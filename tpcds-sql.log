ubuntu@ubuntu-Virtual-Machine:~/git$ git clone https://github.com/maropu/spark-tpcds-datagen.git
Cloning into 'spark-tpcds-datagen'...
remote: Enumerating objects: 4686, done.
remote: Counting objects: 100% (505/505), done.
remote: Compressing objects: 100% (143/143), done.
remote: Total 4686 (delta 333), reused 500 (delta 332), pack-reused 4181
Receiving objects: 100% (4686/4686), 36.14 MiB | 11.39 MiB/s, done.
Resolving deltas: 100% (2760/2760), done.
ubuntu@ubuntu-Virtual-Machine:~/git$ ls
101                                flink-playgrounds  presto             spark-homework
admission-controller-webhook-demo  golang             skaffold           spark-tpcds-datagen
flink-1.15.0                       kind               skaffold_download  SparkWordCount
flink-1.15.0-bin-scala_2.12.tgz    kylin              spark              trino
ubuntu@ubuntu-Virtual-Machine:~/git$ cd spark-tpcds-datagen/
ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ ls
assembly  bin  build  LICENSE  pom.xml  README.md  reports  scalastyle-config.xml  src  thirdparty
ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ wget https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz
--2022-07-24 17:52:21--  https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz
Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2
Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 224374704 (214M) [application/x-gzip]
Saving to: ‘spark-3.1.1-bin-hadoop2.7.tgz’

spark-3.1.1-bin-hadoop2.7.t 100%[==========================================>] 213.98M  9.71MB/s    in 24s     

2022-07-24 17:52:46 (9.09 MB/s) - ‘spark-3.1.1-bin-hadoop2.7.tgz’ saved [224374704/224374704]

ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ tar -zxvf spark-3.1.1-bin-hadoop2.7.tgz 
spark-3.1.1-bin-hadoop2.7/
spark-3.1.1-bin-hadoop2.7/NOTICE
spark-3.1.1-bin-hadoop2.7/kubernetes/
spark-3.1.1-bin-hadoop2.7/kubernetes/tests/
spark-3.1.1-bin-hadoop2.7/kubernetes/tests/python_executable_check.py
spark-3.1.1-bin-hadoop2.7/kubernetes/tests/autoscale.py
spark-3.1.1-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py
spark-3.1.1-bin-hadoop2.7/kubernetes/tests/py_container_checks.py
spark-3.1.1-bin-hadoop2.7/kubernetes/tests/decommissioning.py
spark-3.1.1-bin-hadoop2.7/kubernetes/tests/pyfiles.py
spark-3.1.1-bin-hadoop2.7/kubernetes/tests/decommissioning_cleanup.py
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/decom.sh
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile
spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile
spark-3.1.1-bin-hadoop2.7/jars/
spark-3.1.1-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar
spark-3.1.1-bin-hadoop2.7/jars/RoaringBitmap-0.9.0.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-digester-1.8.jar
spark-3.1.1-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-vector-code-gen-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/derby-10.12.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar
spark-3.1.1-bin-hadoop2.7/jars/okhttp-3.12.12.jar
spark-3.1.1-bin-hadoop2.7/jars/httpcore-4.4.12.jar
spark-3.1.1-bin-hadoop2.7/jars/logging-interceptor-3.12.12.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/scala-library-2.12.10.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-mllib-local_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/parquet-format-2.4.0.jar
spark-3.1.1-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar
spark-3.1.1-bin-hadoop2.7/jars/xercesImpl-2.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-logging-1.1.3.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-compiler-3.0.16.jar
spark-3.1.1-bin-hadoop2.7/jars/jdo-api-3.0.1.jar
spark-3.1.1-bin-hadoop2.7/jars/spire-macros_2.12-0.17.0-M1.jar
spark-3.1.1-bin-hadoop2.7/jars/json4s-core_2.12-3.7.0-M5.jar
spark-3.1.1-bin-hadoop2.7/jars/JLargeArrays-1.5.jar
spark-3.1.1-bin-hadoop2.7/jars/jsp-api-2.1.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-autoscaling-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.30.jar
spark-3.1.1-bin-hadoop2.7/jars/json4s-ast_2.12-3.7.0-M5.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-cli-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar
spark-3.1.1-bin-hadoop2.7/jars/parquet-common-1.10.1.jar
spark-3.1.1-bin-hadoop2.7/jars/stax-api-1.0.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.10.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-storage-api-2.7.2.jar
spark-3.1.1-bin-hadoop2.7/jars/algebra_2.12-2.0.0-M2.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-annotations-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/guice-servlet-3.0.jar
spark-3.1.1-bin-hadoop2.7/jars/spire-util_2.12-0.17.0-M1.jar
spark-3.1.1-bin-hadoop2.7/jars/jakarta.activation-api-1.2.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jline-2.14.6.jar
spark-3.1.1-bin-hadoop2.7/jars/breeze_2.12-1.0.jar
spark-3.1.1-bin-hadoop2.7/jars/metrics-jvm-4.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar
spark-3.1.1-bin-hadoop2.7/jars/machinist_2.12-0.6.8.jar
spark-3.1.1-bin-hadoop2.7/jars/scala-compiler-2.12.10.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-pool-1.5.4.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-network-shuffle_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-repl_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jta-1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-admissionregistration-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/istack-commons-runtime-3.0.8.jar
spark-3.1.1-bin-hadoop2.7/jars/metrics-graphite-4.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-hdfs-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-streaming_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-launcher_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/objenesis-2.6.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-collections-3.2.2.jar
spark-3.1.1-bin-hadoop2.7/jars/audience-annotations-0.5.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jpam-1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jakarta.annotation-api-1.3.5.jar
spark-3.1.1-bin-hadoop2.7/jars/xmlenc-0.52.jar
spark-3.1.1-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-common-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-extensions-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/py4j-0.10.9.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/spire-platform_2.12-0.17.0-M1.jar
spark-3.1.1-bin-hadoop2.7/jars/avro-1.8.2.jar
spark-3.1.1-bin-hadoop2.7/jars/curator-client-2.7.1.jar
spark-3.1.1-bin-hadoop2.7/jars/javax.inject-1.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-apiextensions-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jersey-media-jaxb-2.30.jar
spark-3.1.1-bin-hadoop2.7/jars/javax.jdo-3.2.0-m3.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-auth-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-io-2.4.jar
spark-3.1.1-bin-hadoop2.7/jars/json4s-jackson_2.12-3.7.0-M5.jar
spark-3.1.1-bin-hadoop2.7/jars/paranamer-2.8.jar
spark-3.1.1-bin-hadoop2.7/jars/cats-kernel_2.12-2.0.0-M4.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-mllib_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/flatbuffers-java-1.9.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jersey-server-2.30.jar
spark-3.1.1-bin-hadoop2.7/jars/stream-2.9.6.jar
spark-3.1.1-bin-hadoop2.7/jars/datanucleus-api-jdo-4.2.4.jar
spark-3.1.1-bin-hadoop2.7/jars/gson-2.2.4.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-hive-thriftserver_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/xml-apis-1.4.01.jar
spark-3.1.1-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar
spark-3.1.1-bin-hadoop2.7/jars/antlr4-runtime-4.8-1.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-policy-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-jdbc-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-exec-2.3.7-core.jar
spark-3.1.1-bin-hadoop2.7/jars/opencsv-2.3.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-certificates-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar
spark-3.1.1-bin-hadoop2.7/jars/jsr305-3.0.0.jar
spark-3.1.1-bin-hadoop2.7/jars/macro-compat_2.12-1.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-sketch_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-dbcp-1.4.jar
spark-3.1.1-bin-hadoop2.7/jars/jersey-container-servlet-core-2.30.jar
spark-3.1.1-bin-hadoop2.7/jars/okio-1.14.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hk2-api-2.6.1.jar
spark-3.1.1-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar
spark-3.1.1-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar
spark-3.1.1-bin-hadoop2.7/jars/orc-mapreduce-1.5.12.jar
spark-3.1.1-bin-hadoop2.7/jars/chill-java-0.9.5.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-settings-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-tags_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jakarta.servlet-api-4.0.3.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/automaton-1.11-8.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-lang-2.6.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-annotations-2.10.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hk2-utils-2.6.1.jar
spark-3.1.1-bin-hadoop2.7/jars/velocity-1.5.jar
spark-3.1.1-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar
spark-3.1.1-bin-hadoop2.7/jars/jul-to-slf4j-1.7.30.jar
spark-3.1.1-bin-hadoop2.7/jars/JTransforms-3.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jersey-client-2.30.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-catalyst_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/httpclient-4.5.6.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-discovery-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jetty-sslengine-6.1.26.jar
spark-3.1.1-bin-hadoop2.7/jars/spire_2.12-0.17.0-M1.jar
spark-3.1.1-bin-hadoop2.7/jars/jersey-hk2-2.30.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-sql_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-math3-3.4.1.jar
spark-3.1.1-bin-hadoop2.7/jars/xbean-asm7-shaded-4.15.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-network-common_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jakarta.validation-api-2.0.2.jar
spark-3.1.1-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar
spark-3.1.1-bin-hadoop2.7/jars/snappy-java-1.1.8.2.jar
spark-3.1.1-bin-hadoop2.7/jars/slf4j-log4j12-1.7.30.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/slf4j-api-1.7.30.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-shims-0.23-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/guava-14.0.1.jar
spark-3.1.1-bin-hadoop2.7/jars/stax-api-1.0-2.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-kubernetes_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-service-rpc-3.1.2.jar
spark-3.1.1-bin-hadoop2.7/jars/shapeless_2.12-2.3.3.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-graphx_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/oro-2.0.8.jar
spark-3.1.1-bin-hadoop2.7/jars/arrow-memory-netty-2.0.0.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-scheduling-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/libfb303-0.9.3.jar
spark-3.1.1-bin-hadoop2.7/jars/core-1.1.2.jar
spark-3.1.1-bin-hadoop2.7/jars/jersey-container-servlet-2.30.jar
spark-3.1.1-bin-hadoop2.7/jars/datanucleus-rdbms-4.1.19.jar
spark-3.1.1-bin-hadoop2.7/jars/super-csv-2.2.0.jar
spark-3.1.1-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-module-paranamer-2.10.0.jar
spark-3.1.1-bin-hadoop2.7/jars/aopalliance-1.0.jar
spark-3.1.1-bin-hadoop2.7/jars/osgi-resource-locator-1.0.3.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-module-scala_2.12-2.10.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-shims-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/json-1.8.jar
spark-3.1.1-bin-hadoop2.7/jars/antlr-runtime-3.5.2.jar
spark-3.1.1-bin-hadoop2.7/jars/threeten-extra-1.5.0.jar
spark-3.1.1-bin-hadoop2.7/jars/orc-shims-1.5.12.jar
spark-3.1.1-bin-hadoop2.7/jars/jetty-6.1.26.jar
spark-3.1.1-bin-hadoop2.7/jars/arrow-vector-2.0.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.10.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jersey-common-2.30.jar
spark-3.1.1-bin-hadoop2.7/jars/aircompressor-0.10.jar
spark-3.1.1-bin-hadoop2.7/jars/lz4-java-1.7.1.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-client-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/activation-1.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-crypto-1.1.0.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-tags_2.12-3.1.1-tests.jar
spark-3.1.1-bin-hadoop2.7/jars/libthrift-0.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/HikariCP-2.5.1.jar
spark-3.1.1-bin-hadoop2.7/jars/generex-1.0.2.jar
spark-3.1.1-bin-hadoop2.7/jars/breeze-macros_2.12-1.0.jar
spark-3.1.1-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar
spark-3.1.1-bin-hadoop2.7/jars/jaxb-runtime-2.3.2.jar
spark-3.1.1-bin-hadoop2.7/jars/json4s-scalap_2.12-3.7.0-M5.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-events-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-lang3-3.10.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-datatype-jsr310-2.11.2.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-httpclient-3.1.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-client-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-hive_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/shims-0.9.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar
spark-3.1.1-bin-hadoop2.7/jars/scala-xml_2.12-1.2.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hk2-locator-2.6.1.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-common-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/orc-core-1.5.12.jar
spark-3.1.1-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-common-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/xz-1.5.jar
spark-3.1.1-bin-hadoop2.7/jars/ST4-4.0.4.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-shims-scheduler-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-yarn_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-batch-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/netty-all-4.1.51.Final.jar
spark-3.1.1-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar
spark-3.1.1-bin-hadoop2.7/jars/chill_2.12-0.9.5.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-databind-2.10.0.jar
spark-3.1.1-bin-hadoop2.7/jars/scala-reflect-2.12.10.jar
spark-3.1.1-bin-hadoop2.7/jars/joda-time-2.10.5.jar
spark-3.1.1-bin-hadoop2.7/jars/minlog-1.3.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jakarta.ws.rs-api-2.1.6.jar
spark-3.1.1-bin-hadoop2.7/jars/aopalliance-repackaged-2.6.1.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-compress-1.20.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-core_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar
spark-3.1.1-bin-hadoop2.7/jars/jodd-core-3.5.2.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-storageclass-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-core-2.10.0.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-core-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-net-3.1.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/datanucleus-core-4.1.17.jar
spark-3.1.1-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar
spark-3.1.1-bin-hadoop2.7/jars/janino-3.0.16.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-shims-common-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/javolution-5.5.1.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-rbac-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/transaction-api-1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-metrics-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-cli-1.2.jar
spark-3.1.1-bin-hadoop2.7/jars/jakarta.inject-2.6.1.jar
spark-3.1.1-bin-hadoop2.7/jars/metrics-jmx-4.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/zstd-jni-1.4.8-1.jar
spark-3.1.1-bin-hadoop2.7/jars/jakarta.xml.bind-api-2.3.2.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-codec-1.10.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/curator-framework-2.7.1.jar
spark-3.1.1-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-metastore-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-beeline-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/metrics-core-4.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-llap-common-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/arrow-format-2.0.0.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-kvstore_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/javassist-3.25.0-GA.jar
spark-3.1.1-bin-hadoop2.7/jars/scala-collection-compat_2.12-2.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/arrow-memory-core-2.0.0.jar
spark-3.1.1-bin-hadoop2.7/jars/parquet-column-1.10.1.jar
spark-3.1.1-bin-hadoop2.7/jars/jetty-util-6.1.26.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-coordination-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar
spark-3.1.1-bin-hadoop2.7/jars/guice-3.0.jar
spark-3.1.1-bin-hadoop2.7/jars/commons-text-1.6.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-apps-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/metrics-json-4.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/snakeyaml-1.24.jar
spark-3.1.1-bin-hadoop2.7/jars/ivy-2.4.0.jar
spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.4.jar
spark-3.1.1-bin-hadoop2.7/jars/univocity-parsers-2.9.1.jar
spark-3.1.1-bin-hadoop2.7/jars/pyrolite-4.30.jar
spark-3.1.1-bin-hadoop2.7/jars/spark-mesos_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/jars/log4j-1.2.17.jar
spark-3.1.1-bin-hadoop2.7/jars/hive-serde-2.3.7.jar
spark-3.1.1-bin-hadoop2.7/jars/scala-parser-combinators_2.12-1.1.2.jar
spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-networking-4.12.0.jar
spark-3.1.1-bin-hadoop2.7/jars/zookeeper-3.4.14.jar
spark-3.1.1-bin-hadoop2.7/data/
spark-3.1.1-bin-hadoop2.7/data/mllib/
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_lda_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_svm_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/iris_libsvm.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_movielens_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/als/
spark-3.1.1-bin-hadoop2.7/data/mllib/als/test.data
spark-3.1.1-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/pic_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/images/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/license.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg
spark-3.1.1-bin-hadoop2.7/data/mllib/images/license.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/ridge-data/
spark-3.1.1-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data
spark-3.1.1-bin-hadoop2.7/data/mllib/kmeans_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/pagerank_data.txt
spark-3.1.1-bin-hadoop2.7/data/mllib/gmm_data.txt
spark-3.1.1-bin-hadoop2.7/data/graphx/
spark-3.1.1-bin-hadoop2.7/data/graphx/users.txt
spark-3.1.1-bin-hadoop2.7/data/graphx/followers.txt
spark-3.1.1-bin-hadoop2.7/data/streaming/
spark-3.1.1-bin-hadoop2.7/data/streaming/AFINN-111.txt
spark-3.1.1-bin-hadoop2.7/R/
spark-3.1.1-bin-hadoop2.7/R/lib/
spark-3.1.1-bin-hadoop2.7/R/lib/sparkr.zip
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/tests/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/tests/testthat/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/profile/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/profile/shell.R
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/profile/general.R
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/INDEX
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/AnIndex
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/paths.rds
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/R/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/R/SparkR
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/NAMESPACE
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/html/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/html/00Index.html
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/html/R.css
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/worker/
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/worker/worker.R
spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R
spark-3.1.1-bin-hadoop2.7/README.md
spark-3.1.1-bin-hadoop2.7/RELEASE
spark-3.1.1-bin-hadoop2.7/yarn/
spark-3.1.1-bin-hadoop2.7/yarn/spark-3.1.1-yarn-shuffle.jar
spark-3.1.1-bin-hadoop2.7/LICENSE
spark-3.1.1-bin-hadoop2.7/sbin/
spark-3.1.1-bin-hadoop2.7/sbin/start-workers.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-master.sh
spark-3.1.1-bin-hadoop2.7/sbin/workers.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-worker.sh
spark-3.1.1-bin-hadoop2.7/sbin/spark-config.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-history-server.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-slaves.sh
spark-3.1.1-bin-hadoop2.7/sbin/spark-daemon.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-worker.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh
spark-3.1.1-bin-hadoop2.7/sbin/decommission-worker.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh
spark-3.1.1-bin-hadoop2.7/sbin/decommission-slave.sh
spark-3.1.1-bin-hadoop2.7/sbin/slaves.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-history-server.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-thriftserver.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-thriftserver.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-slave.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-all.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-slave.sh
spark-3.1.1-bin-hadoop2.7/sbin/spark-daemons.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-workers.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-slaves.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-all.sh
spark-3.1.1-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh
spark-3.1.1-bin-hadoop2.7/sbin/stop-master.sh
spark-3.1.1-bin-hadoop2.7/examples/
spark-3.1.1-bin-hadoop2.7/examples/src/
spark-3.1.1-bin-hadoop2.7/examples/src/main/
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/survreg.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/glm.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/prefixSpan.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/lm_with_elastic_net.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/powerIterationClustering.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/lda.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/kstest.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/ml.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/fmRegressor.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/mlp.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/als.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/fmClassifier.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/logit.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/gbt.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/fpm.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/dataframe.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/data-manipulation.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/streaming/
spark-3.1.1-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/people.json
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/users.avro
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/people.csv
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/users.parquet
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/users.orc
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/file1.parquet
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/file2.parquet
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/file3.json
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/user.avsc
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/full_user.avsc
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/kv1.txt
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/people.txt
spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/employees.json
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java
spark-3.1.1-bin-hadoop2.7/examples/src/main/scripts/
spark-3.1.1-bin-hadoop2.7/examples/src/main/scripts/getGpusResources.sh
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/kmeans.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/fm_regressor_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/univariate_feature_selector_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/robust_scaler_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/power_iteration_clustering_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/als_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/fm_classifier_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/variance_threshold_selector_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/interaction_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/logistic_regression.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/als.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/wordcount.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/status_api_demo.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/pagerank.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sort.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/transitive_closure.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/pi.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/datasource.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/hive.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/arrow.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/streaming/
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/basic.py
spark-3.1.1-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py
spark-3.1.1-bin-hadoop2.7/examples/jars/
spark-3.1.1-bin-hadoop2.7/examples/jars/spark-examples_2.12-3.1.1.jar
spark-3.1.1-bin-hadoop2.7/examples/jars/scopt_2.12-3.7.1.jar
spark-3.1.1-bin-hadoop2.7/conf/
spark-3.1.1-bin-hadoop2.7/conf/metrics.properties.template
spark-3.1.1-bin-hadoop2.7/conf/workers.template
spark-3.1.1-bin-hadoop2.7/conf/fairscheduler.xml.template
spark-3.1.1-bin-hadoop2.7/conf/log4j.properties.template
spark-3.1.1-bin-hadoop2.7/conf/spark-defaults.conf.template
spark-3.1.1-bin-hadoop2.7/conf/spark-env.sh.template
spark-3.1.1-bin-hadoop2.7/bin/
spark-3.1.1-bin-hadoop2.7/bin/sparkR.cmd
spark-3.1.1-bin-hadoop2.7/bin/sparkR
spark-3.1.1-bin-hadoop2.7/bin/spark-submit
spark-3.1.1-bin-hadoop2.7/bin/pyspark2.cmd
spark-3.1.1-bin-hadoop2.7/bin/spark-class
spark-3.1.1-bin-hadoop2.7/bin/pyspark.cmd
spark-3.1.1-bin-hadoop2.7/bin/spark-submit2.cmd
spark-3.1.1-bin-hadoop2.7/bin/load-spark-env.cmd
spark-3.1.1-bin-hadoop2.7/bin/spark-sql
spark-3.1.1-bin-hadoop2.7/bin/docker-image-tool.sh
spark-3.1.1-bin-hadoop2.7/bin/find-spark-home.cmd
spark-3.1.1-bin-hadoop2.7/bin/load-spark-env.sh
spark-3.1.1-bin-hadoop2.7/bin/pyspark
spark-3.1.1-bin-hadoop2.7/bin/spark-shell.cmd
spark-3.1.1-bin-hadoop2.7/bin/spark-shell2.cmd
spark-3.1.1-bin-hadoop2.7/bin/spark-submit.cmd
spark-3.1.1-bin-hadoop2.7/bin/beeline.cmd
spark-3.1.1-bin-hadoop2.7/bin/find-spark-home
spark-3.1.1-bin-hadoop2.7/bin/spark-class.cmd
spark-3.1.1-bin-hadoop2.7/bin/sparkR2.cmd
spark-3.1.1-bin-hadoop2.7/bin/beeline
spark-3.1.1-bin-hadoop2.7/bin/spark-class2.cmd
spark-3.1.1-bin-hadoop2.7/bin/spark-sql.cmd
spark-3.1.1-bin-hadoop2.7/bin/run-example
spark-3.1.1-bin-hadoop2.7/bin/spark-shell
spark-3.1.1-bin-hadoop2.7/bin/run-example.cmd
spark-3.1.1-bin-hadoop2.7/bin/spark-sql2.cmd
spark-3.1.1-bin-hadoop2.7/python/
spark-3.1.1-bin-hadoop2.7/python/.gitignore
spark-3.1.1-bin-hadoop2.7/python/run-tests-with-coverage
spark-3.1.1-bin-hadoop2.7/python/mypy.ini
spark-3.1.1-bin-hadoop2.7/python/pylintrc
spark-3.1.1-bin-hadoop2.7/python/MANIFEST.in
spark-3.1.1-bin-hadoop2.7/python/README.md
spark-3.1.1-bin-hadoop2.7/python/test_coverage/
spark-3.1.1-bin-hadoop2.7/python/test_coverage/coverage_daemon.py
spark-3.1.1-bin-hadoop2.7/python/test_coverage/conf/
spark-3.1.1-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf
spark-3.1.1-bin-hadoop2.7/python/test_coverage/sitecustomize.py
spark-3.1.1-bin-hadoop2.7/python/run-tests.py
spark-3.1.1-bin-hadoop2.7/python/setup.py
spark-3.1.1-bin-hadoop2.7/python/test_support/
spark-3.1.1-bin-hadoop2.7/python/test_support/userlibrary.py
spark-3.1.1-bin-hadoop2.7/python/test_support/hello/
spark-3.1.1-bin-hadoop2.7/python/test_support/hello/sub_hello/
spark-3.1.1-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt
spark-3.1.1-bin-hadoop2.7/python/test_support/hello/hello.txt
spark-3.1.1-bin-hadoop2.7/python/test_support/userlib-0.1.zip
spark-3.1.1-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/people.json
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/people_array.json
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/text-test.txt
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/ages.csv
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/streaming/
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt
spark-3.1.1-bin-hadoop2.7/python/test_support/sql/people1.json
spark-3.1.1-bin-hadoop2.7/python/pyspark/
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_rddbarrier.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_worker.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_serializers.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_util.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_rdd.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_broadcast.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_appsubmit.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_profiler.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_pin_thread.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_install_spark.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_shuffle.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_join.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_taskcontext.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_context.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_readwrite.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_conf.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_daemon.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/__init__.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/_typing.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/
spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/mlutils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/mllibutils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/utils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/sqlutils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/streamingutils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/accumulators.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/rddsampler.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/install.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/status.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_algorithms.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_evaluation.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_util.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_feature.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_pipeline.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_wrapper.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_tuning.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_persistence.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_param.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_training_summary.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_linalg.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_image.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_stat.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_base.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/evaluation.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/_typing.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/functions.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/recommendation.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/fpm.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/base.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/feature.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/stat.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/stat.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/image.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/classification.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/common.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/recommendation.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/clustering.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/regression.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/__init__.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/shared.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/shared.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/feature.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/classification.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tree.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/util.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/fpm.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/regression.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/functions.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/base.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/image.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tree.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/clustering.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/common.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/linalg/
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/evaluation.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/util.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/context.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/find_spark_home.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/serializers.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/java_gateway.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/traceback_utils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/tests/
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/tests/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/tests/test_resources.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/profile.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/information.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/requests.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/information.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/requests.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/profile.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/conf.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resultiterable.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/version.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/files.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_algorithms.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_streaming_algorithms.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_util.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_feature.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_linalg.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_stat.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/evaluation.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/_typing.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/recommendation.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/fpm.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/feature.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/classification.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/common.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/random.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/recommendation.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/clustering.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/regression.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/feature.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/classification.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tree.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/util.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/fpm.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/regression.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/random.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/test.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/test.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tree.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/clustering.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/common.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/evaluation.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/util.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/resultiterable.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/profiler.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/statcounter.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/join.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/daemon.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/rdd.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/context.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/
spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/cloudpickle_fast.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/compat.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/cloudpickle.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/storagelevel.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/version.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/py.typed
spark-3.1.1-bin-hadoop2.7/python/pyspark/files.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/worker.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/statcounter.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/conf.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/shell.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/test_listener.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/test_kinesis.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/test_dstream.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/test_context.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/dstream.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/kinesis.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/context.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/dstream.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/listener.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/kinesis.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/listener.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/context.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/util.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/accumulators.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/profiler.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/status.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/broadcast.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_functions.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_readwriter.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_utils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_grouped_map.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_dataframe.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_map.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_udf.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_streaming.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_serde.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_window.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_group.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_cogrouped_map.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_scalar.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_catalog.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_datasources.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_typehints.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_types.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_column.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_context.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_conf.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_arrow.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_session.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/__init__.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/map_ops.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/functions.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/serializers.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/typehints.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/map_ops.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/types.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/functions.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/utils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/__init__.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/series.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/frame.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/_typing.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/streaming.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/context.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/column.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/catalog.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/types.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/window.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/udf.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/conf.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/session.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/column.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/group.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/catalog.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/group.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/context.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/types.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/functions.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/conf.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/udf.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/avro/
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/avro/functions.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/avro/__init__.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/avro/functions.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/readwriter.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/window.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/streaming.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/shuffle.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/rdd.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/taskcontext.pyi
spark-3.1.1-bin-hadoop2.7/python/pyspark/taskcontext.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/_globals.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/broadcast.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/util.py
spark-3.1.1-bin-hadoop2.7/python/pyspark/storagelevel.py
spark-3.1.1-bin-hadoop2.7/python/.coveragerc
spark-3.1.1-bin-hadoop2.7/python/docs/
spark-3.1.1-bin-hadoop2.7/python/docs/make2.bat
spark-3.1.1-bin-hadoop2.7/python/docs/source/
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.ss.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/index.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.ml.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.mllib.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.streaming.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.sql.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.resource.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/index.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/getting_started/
spark-3.1.1-bin-hadoop2.7/python/docs/source/getting_started/index.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/getting_started/quickstart.ipynb
spark-3.1.1-bin-hadoop2.7/python/docs/source/getting_started/install.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/conf.py
spark-3.1.1-bin-hadoop2.7/python/docs/source/_templates/
spark-3.1.1-bin-hadoop2.7/python/docs/source/_templates/autosummary/
spark-3.1.1-bin-hadoop2.7/python/docs/source/_templates/autosummary/class.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/_templates/autosummary/class_with_docs.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/_static/
spark-3.1.1-bin-hadoop2.7/python/docs/source/_static/copybutton.js
spark-3.1.1-bin-hadoop2.7/python/docs/source/_static/css/
spark-3.1.1-bin-hadoop2.7/python/docs/source/_static/css/pyspark.css
spark-3.1.1-bin-hadoop2.7/python/docs/source/development/
spark-3.1.1-bin-hadoop2.7/python/docs/source/development/setting_ide.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/development/index.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/development/debugging.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/development/testing.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/development/contributing.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/user_guide/
spark-3.1.1-bin-hadoop2.7/python/docs/source/user_guide/arrow_pandas.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/user_guide/index.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/user_guide/python_packaging.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/
spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/index.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst
spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst
spark-3.1.1-bin-hadoop2.7/python/docs/make.bat
spark-3.1.1-bin-hadoop2.7/python/docs/Makefile
spark-3.1.1-bin-hadoop2.7/python/lib/
spark-3.1.1-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt
spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip
spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip
spark-3.1.1-bin-hadoop2.7/python/run-tests
spark-3.1.1-bin-hadoop2.7/python/setup.cfg
spark-3.1.1-bin-hadoop2.7/licenses/
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-respond.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-antlr.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-janino.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-protobuf.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jquery.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-scopt.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-netlib.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-datatables.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-paranamer.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jakarta-ws-rs-api
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-dnsjava.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jakarta.xml.bind-api.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jakarta-annotation-api
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-CC0.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jodd.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-f2j.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-machinist.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-javolution.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-modernizr.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-spire.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-join.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-slf4j.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-arpack.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jsp-api.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-JTransforms.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-JLargeArrays.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-javassist.html
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-zstd.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-scala.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jakarta.activation-api.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-automaton.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-javax-transaction-transaction-api.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jaxb-runtime.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-minlog.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-mustache.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jline.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-istack-commons-runtime.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-py4j.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-vis-timeline.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-re2j.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-kryo.txt
spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt
ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ ls
assembly  build    pom.xml    reports                spark-3.1.1-bin-hadoop2.7      src
bin       LICENSE  README.md  scalastyle-config.xml  spark-3.1.1-bin-hadoop2.7.tgz  thirdparty
ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ mkdir -p tpcds-data-1g
ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ export SPARK_HOME=./spark-3.1.1-bin-hadoop2.7
ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ ./bin/dsdgen --output-location tpcds-data-1g
/home/ubuntu/git/spark-tpcds-datagen/bin/../target/spark-tpcds-datagen_2.12-0.1.0-SNAPSHOT-with-dependencies.jar not found, so use pre-compiled /home/ubuntu/git/spark-tpcds-datagen/bin/../assembly/spark-tpcds-datagen_2.12-0.1.0-SNAPSHOT-with-dependencies.jar
Using `spark-submit` from path: ./spark-3.1.1-bin-hadoop2.7
22/07/24 17:58:22 WARN Utils: Your hostname, ubuntu-Virtual-Machine resolves to a loopback address: 127.0.1.1; using 172.28.45.111 instead (on interface eth0)
22/07/24 17:58:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/07/24 17:58:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/07/24 17:58:23 INFO SparkContext: Running Spark version 3.1.1
22/07/24 17:58:23 INFO ResourceUtils: ==============================================================
22/07/24 17:58:23 INFO ResourceUtils: No custom resources configured for spark.driver.
22/07/24 17:58:23 INFO ResourceUtils: ==============================================================
22/07/24 17:58:23 INFO SparkContext: Submitted application: org.apache.spark.sql.execution.benchmark.TPCDSDatagen
22/07/24 17:58:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/07/24 17:58:23 INFO ResourceProfile: Limiting resource is cpu
22/07/24 17:58:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/07/24 17:58:23 INFO SecurityManager: Changing view acls to: ubuntu
22/07/24 17:58:23 INFO SecurityManager: Changing modify acls to: ubuntu
22/07/24 17:58:23 INFO SecurityManager: Changing view acls groups to: 
22/07/24 17:58:23 INFO SecurityManager: Changing modify acls groups to: 
22/07/24 17:58:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ubuntu); groups with view permissions: Set(); users  with modify permissions: Set(ubuntu); groups with modify permissions: Set()
22/07/24 17:58:24 INFO Utils: Successfully started service 'sparkDriver' on port 44383.
22/07/24 17:58:24 INFO SparkEnv: Registering MapOutputTracker
22/07/24 17:58:24 INFO SparkEnv: Registering BlockManagerMaster
22/07/24 17:58:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/07/24 17:58:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/07/24 17:58:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/07/24 17:58:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-17404399-720e-4cb5-a8df-d41753170c31
22/07/24 17:58:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
22/07/24 17:58:24 INFO SparkEnv: Registering OutputCommitCoordinator
22/07/24 17:58:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/07/24 17:58:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ubuntu-Virtual-Machine.mshome.net:4040
22/07/24 17:58:24 INFO SparkContext: Added JAR file:/home/ubuntu/git/spark-tpcds-datagen/bin/../assembly/spark-tpcds-datagen_2.12-0.1.0-SNAPSHOT-with-dependencies.jar at spark://ubuntu-Virtual-Machine.mshome.net:44383/jars/spark-tpcds-datagen_2.12-0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1658656703542
22/07/24 17:58:24 INFO Executor: Starting executor ID driver on host ubuntu-Virtual-Machine.mshome.net
22/07/24 17:58:24 INFO Executor: Fetching spark://ubuntu-Virtual-Machine.mshome.net:44383/jars/spark-tpcds-datagen_2.12-0.1.0-SNAPSHOT-with-dependencies.jar with timestamp 1658656703542
22/07/24 17:58:24 INFO TransportClientFactory: Successfully created connection to ubuntu-Virtual-Machine.mshome.net/172.28.45.111:44383 after 34 ms (0 ms spent in bootstraps)
22/07/24 17:58:24 INFO Utils: Fetching spark://ubuntu-Virtual-Machine.mshome.net:44383/jars/spark-tpcds-datagen_2.12-0.1.0-SNAPSHOT-with-dependencies.jar to /tmp/spark-fe9b1ba4-3ef1-4061-ad6d-b15f9243ba66/userFiles-8c04bc04-8299-4d8a-8d33-71ff8593fb55/fetchFileTemp5532151666400986914.tmp
22/07/24 17:58:24 INFO Executor: Adding file:/tmp/spark-fe9b1ba4-3ef1-4061-ad6d-b15f9243ba66/userFiles-8c04bc04-8299-4d8a-8d33-71ff8593fb55/spark-tpcds-datagen_2.12-0.1.0-SNAPSHOT-with-dependencies.jar to class loader
22/07/24 17:58:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38047.
22/07/24 17:58:24 INFO NettyBlockTransferService: Server created on ubuntu-Virtual-Machine.mshome.net:38047
22/07/24 17:58:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/07/24 17:58:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu-Virtual-Machine.mshome.net, 38047, None)
22/07/24 17:58:24 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu-Virtual-Machine.mshome.net:38047 with 366.3 MiB RAM, BlockManagerId(driver, ubuntu-Virtual-Machine.mshome.net, 38047, None)
22/07/24 17:58:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu-Virtual-Machine.mshome.net, 38047, None)
22/07/24 17:58:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu-Virtual-Machine.mshome.net, 38047, None)
22/07/24 17:58:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/ubuntu/git/spark-tpcds-datagen/spark-warehouse').
22/07/24 17:58:25 INFO SharedState: Warehouse path is 'file:/home/ubuntu/git/spark-tpcds-datagen/spark-warehouse'.
22/07/24 17:58:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
22/07/24 17:58:28 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:58:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:58:28 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:58:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:58:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:58:29 INFO CodeGenerator: Code generated in 286.774905 ms
22/07/24 17:58:29 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 17:58:29 INFO DAGScheduler: Got job 0 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 17:58:29 INFO DAGScheduler: Final stage: ResultStage 0 (main at NativeMethodAccessorImpl.java:0)
22/07/24 17:58:29 INFO DAGScheduler: Parents of final stage: List()
22/07/24 17:58:29 INFO DAGScheduler: Missing parents: List()
22/07/24 17:58:29 INFO DAGScheduler: Submitting ResultStage 0 (CoalescedRDD[6] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 17:58:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 234.0 KiB, free 366.1 MiB)
22/07/24 17:58:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 71.0 KiB, free 366.0 MiB)
22/07/24 17:58:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 71.0 KiB, free: 366.2 MiB)
22/07/24 17:58:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1383
22/07/24 17:58:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (CoalescedRDD[6] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 17:58:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
22/07/24 17:58:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 17:58:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
22/07/24 17:58:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:58:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:58:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:58:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:58:30 INFO CodecConfig: Compression: SNAPPY
22/07/24 17:58:30 INFO CodecConfig: Compression: SNAPPY
22/07/24 17:58:30 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 17:58:30 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 17:58:30 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 17:58:30 INFO ParquetOutputFormat: Dictionary is on
22/07/24 17:58:30 INFO ParquetOutputFormat: Validation is off
22/07/24 17:58:30 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 17:58:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 17:58:30 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 17:58:30 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 17:58:30 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 17:58:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "cs_sold_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_sold_time_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ship_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_bill_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_bill_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_bill_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_bill_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ship_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ship_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ship_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ship_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_call_center_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_catalog_page_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ship_mode_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_warehouse_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_promo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_order_number",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_quantity",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_wholesale_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_list_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_sales_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ext_discount_amt",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ext_sales_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ext_wholesale_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ext_list_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ext_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_coupon_amt",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_ext_ship_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_net_paid",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_net_paid_inc_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_net_paid_inc_ship",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_net_paid_inc_ship_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cs_net_profit",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 cs_sold_date_sk;
  optional int32 cs_sold_time_sk;
  optional int32 cs_ship_date_sk;
  optional int32 cs_bill_customer_sk;
  optional int32 cs_bill_cdemo_sk;
  optional int32 cs_bill_hdemo_sk;
  optional int32 cs_bill_addr_sk;
  optional int32 cs_ship_customer_sk;
  optional int32 cs_ship_cdemo_sk;
  optional int32 cs_ship_hdemo_sk;
  optional int32 cs_ship_addr_sk;
  optional int32 cs_call_center_sk;
  optional int32 cs_catalog_page_sk;
  optional int32 cs_ship_mode_sk;
  optional int32 cs_warehouse_sk;
  optional int32 cs_item_sk;
  optional int32 cs_promo_sk;
  optional int32 cs_order_number;
  optional int32 cs_quantity;
  optional int32 cs_wholesale_cost (DECIMAL(7,2));
  optional int32 cs_list_price (DECIMAL(7,2));
  optional int32 cs_sales_price (DECIMAL(7,2));
  optional int32 cs_ext_discount_amt (DECIMAL(7,2));
  optional int32 cs_ext_sales_price (DECIMAL(7,2));
  optional int32 cs_ext_wholesale_cost (DECIMAL(7,2));
  optional int32 cs_ext_list_price (DECIMAL(7,2));
  optional int32 cs_ext_tax (DECIMAL(7,2));
  optional int32 cs_coupon_amt (DECIMAL(7,2));
  optional int32 cs_ext_ship_cost (DECIMAL(7,2));
  optional int32 cs_net_paid (DECIMAL(7,2));
  optional int32 cs_net_paid_inc_tax (DECIMAL(7,2));
  optional int32 cs_net_paid_inc_ship (DECIMAL(7,2));
  optional int32 cs_net_paid_inc_ship_tax (DECIMAL(7,2));
  optional int32 cs_net_profit (DECIMAL(7,2));
}

       
22/07/24 17:58:30 INFO CodecPool: Got brand-new compressor [.snappy]
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 17:58:30 INFO CodeGenerator: Code generated in 264.56658 ms
22/07/24 17:59:01 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 107694901
22/07/24 17:59:01 INFO FileOutputCommitter: Saved output of task 'attempt_202207241758298517003867272127575_0000_m_000000_0' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/catalog_sales/_temporary/0/task_202207241758298517003867272127575_0000_m_000000
22/07/24 17:59:01 INFO SparkHadoopMapRedUtil: attempt_202207241758298517003867272127575_0000_m_000000_0: Committed
22/07/24 17:59:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2439 bytes result sent to driver
22/07/24 17:59:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 32238 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 17:59:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
22/07/24 17:59:01 INFO DAGScheduler: ResultStage 0 (main at NativeMethodAccessorImpl.java:0) finished in 32.569 s
22/07/24 17:59:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 17:59:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/07/24 17:59:01 INFO DAGScheduler: Job 0 finished: main at NativeMethodAccessorImpl.java:0, took 32.626199 s
22/07/24 17:59:02 INFO FileFormatWriter: Write Job 767520bf-f6bc-4361-accf-a7e67c4cb91a committed.
22/07/24 17:59:02 INFO FileFormatWriter: Finished processing stats for write job 767520bf-f6bc-4361-accf-a7e67c4cb91a.
22/07/24 17:59:02 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:02 INFO CodeGenerator: Code generated in 43.809114 ms
22/07/24 17:59:02 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 17:59:02 INFO DAGScheduler: Got job 1 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 17:59:02 INFO DAGScheduler: Final stage: ResultStage 1 (main at NativeMethodAccessorImpl.java:0)
22/07/24 17:59:02 INFO DAGScheduler: Parents of final stage: List()
22/07/24 17:59:02 INFO DAGScheduler: Missing parents: List()
22/07/24 17:59:02 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[15] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 17:59:02 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 223.0 KiB, free 365.8 MiB)
22/07/24 17:59:02 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 69.2 KiB, free 365.7 MiB)
22/07/24 17:59:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 69.2 KiB, free: 366.2 MiB)
22/07/24 17:59:02 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383
22/07/24 17:59:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[15] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 17:59:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
22/07/24 17:59:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 17:59:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
22/07/24 17:59:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:02 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:02 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:02 INFO CodecConfig: Compression: SNAPPY
22/07/24 17:59:02 INFO CodecConfig: Compression: SNAPPY
22/07/24 17:59:02 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 17:59:02 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 17:59:02 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 17:59:02 INFO ParquetOutputFormat: Dictionary is on
22/07/24 17:59:02 INFO ParquetOutputFormat: Validation is off
22/07/24 17:59:02 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 17:59:02 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 17:59:02 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 17:59:02 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 17:59:02 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 17:59:02 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "cr_returned_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_returned_time_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_refunded_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_refunded_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_refunded_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_refunded_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_returning_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_returning_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_returning_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_returning_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_call_center_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_catalog_page_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_ship_mode_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_warehouse_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_reason_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_order_number",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_return_quantity",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_return_amount",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_return_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_return_amt_inc_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_fee",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_return_ship_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_refunded_cash",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_reversed_charge",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_store_credit",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cr_net_loss",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 cr_returned_date_sk;
  optional int32 cr_returned_time_sk;
  optional int32 cr_item_sk;
  optional int32 cr_refunded_customer_sk;
  optional int32 cr_refunded_cdemo_sk;
  optional int32 cr_refunded_hdemo_sk;
  optional int32 cr_refunded_addr_sk;
  optional int32 cr_returning_customer_sk;
  optional int32 cr_returning_cdemo_sk;
  optional int32 cr_returning_hdemo_sk;
  optional int32 cr_returning_addr_sk;
  optional int32 cr_call_center_sk;
  optional int32 cr_catalog_page_sk;
  optional int32 cr_ship_mode_sk;
  optional int32 cr_warehouse_sk;
  optional int32 cr_reason_sk;
  optional int32 cr_order_number;
  optional int32 cr_return_quantity;
  optional int32 cr_return_amount (DECIMAL(7,2));
  optional int32 cr_return_tax (DECIMAL(7,2));
  optional int32 cr_return_amt_inc_tax (DECIMAL(7,2));
  optional int32 cr_fee (DECIMAL(7,2));
  optional int32 cr_return_ship_cost (DECIMAL(7,2));
  optional int32 cr_refunded_cash (DECIMAL(7,2));
  optional int32 cr_reversed_charge (DECIMAL(7,2));
  optional int32 cr_store_credit (DECIMAL(7,2));
  optional int32 cr_net_loss (DECIMAL(7,2));
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 17:59:02 INFO CodeGenerator: Code generated in 98.803148 ms
22/07/24 17:59:10 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 20485980
22/07/24 17:59:10 INFO FileOutputCommitter: Saved output of task 'attempt_202207241759027642905203811657318_0001_m_000000_1' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/catalog_returns/_temporary/0/task_202207241759027642905203811657318_0001_m_000000
22/07/24 17:59:10 INFO SparkHadoopMapRedUtil: attempt_202207241759027642905203811657318_0001_m_000000_1: Committed
22/07/24 17:59:10 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2396 bytes result sent to driver
22/07/24 17:59:10 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 8065 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 17:59:10 INFO DAGScheduler: ResultStage 1 (main at NativeMethodAccessorImpl.java:0) finished in 8.142 s
22/07/24 17:59:10 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 17:59:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
22/07/24 17:59:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
22/07/24 17:59:10 INFO DAGScheduler: Job 1 finished: main at NativeMethodAccessorImpl.java:0, took 8.156436 s
22/07/24 17:59:10 INFO FileFormatWriter: Write Job 902f90dd-7b53-406e-8c56-c53e6e547f0d committed.
22/07/24 17:59:10 INFO FileFormatWriter: Finished processing stats for write job 902f90dd-7b53-406e-8c56-c53e6e547f0d.
22/07/24 17:59:10 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:10 INFO CodeGenerator: Code generated in 20.576108 ms
22/07/24 17:59:10 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 17:59:10 INFO DAGScheduler: Got job 2 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 17:59:10 INFO DAGScheduler: Final stage: ResultStage 2 (main at NativeMethodAccessorImpl.java:0)
22/07/24 17:59:10 INFO DAGScheduler: Parents of final stage: List()
22/07/24 17:59:10 INFO DAGScheduler: Missing parents: List()
22/07/24 17:59:10 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[24] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 17:59:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 186.9 KiB, free 365.5 MiB)
22/07/24 17:59:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 63.7 KiB, free 365.5 MiB)
22/07/24 17:59:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 63.7 KiB, free: 366.1 MiB)
22/07/24 17:59:10 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1383
22/07/24 17:59:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[24] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 17:59:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
22/07/24 17:59:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 17:59:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
22/07/24 17:59:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:10 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:10 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:10 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:10 INFO CodecConfig: Compression: SNAPPY
22/07/24 17:59:10 INFO CodecConfig: Compression: SNAPPY
22/07/24 17:59:10 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 17:59:10 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 17:59:10 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 17:59:10 INFO ParquetOutputFormat: Dictionary is on
22/07/24 17:59:10 INFO ParquetOutputFormat: Validation is off
22/07/24 17:59:10 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 17:59:10 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 17:59:10 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 17:59:10 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 17:59:10 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 17:59:10 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "inv_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "inv_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "inv_warehouse_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "inv_quantity_on_hand",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 inv_date_sk;
  optional int32 inv_item_sk;
  optional int32 inv_warehouse_sk;
  optional int32 inv_quantity_on_hand;
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 17:59:10 INFO CodeGenerator: Code generated in 39.121325 ms
22/07/24 17:59:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 69.2 KiB, free: 366.2 MiB)
22/07/24 17:59:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 71.0 KiB, free: 366.2 MiB)
22/07/24 17:59:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38555812
22/07/24 17:59:40 INFO FileOutputCommitter: Saved output of task 'attempt_202207241759108872275499926363710_0002_m_000000_2' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/inventory/_temporary/0/task_202207241759108872275499926363710_0002_m_000000
22/07/24 17:59:40 INFO SparkHadoopMapRedUtil: attempt_202207241759108872275499926363710_0002_m_000000_2: Committed
22/07/24 17:59:40 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2396 bytes result sent to driver
22/07/24 17:59:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 29505 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 17:59:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
22/07/24 17:59:40 INFO DAGScheduler: ResultStage 2 (main at NativeMethodAccessorImpl.java:0) finished in 29.562 s
22/07/24 17:59:40 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 17:59:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
22/07/24 17:59:40 INFO DAGScheduler: Job 2 finished: main at NativeMethodAccessorImpl.java:0, took 29.566746 s
22/07/24 17:59:40 INFO FileFormatWriter: Write Job d783dcb1-8282-4461-a1e6-392801e8a69b committed.
22/07/24 17:59:40 INFO FileFormatWriter: Finished processing stats for write job d783dcb1-8282-4461-a1e6-392801e8a69b.
22/07/24 17:59:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:40 INFO CodeGenerator: Code generated in 15.805696 ms
22/07/24 17:59:40 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 17:59:40 INFO DAGScheduler: Got job 3 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 17:59:40 INFO DAGScheduler: Final stage: ResultStage 3 (main at NativeMethodAccessorImpl.java:0)
22/07/24 17:59:40 INFO DAGScheduler: Parents of final stage: List()
22/07/24 17:59:40 INFO DAGScheduler: Missing parents: List()
22/07/24 17:59:40 INFO DAGScheduler: Submitting ResultStage 3 (CoalescedRDD[33] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 17:59:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 216.7 KiB, free 365.8 MiB)
22/07/24 17:59:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 68.5 KiB, free 365.8 MiB)
22/07/24 17:59:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 68.5 KiB, free: 366.2 MiB)
22/07/24 17:59:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1383
22/07/24 17:59:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (CoalescedRDD[33] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 17:59:40 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
22/07/24 17:59:40 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 17:59:40 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
22/07/24 17:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 17:59:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 17:59:40 INFO CodecConfig: Compression: SNAPPY
22/07/24 17:59:40 INFO CodecConfig: Compression: SNAPPY
22/07/24 17:59:40 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 17:59:40 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 17:59:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 17:59:40 INFO ParquetOutputFormat: Dictionary is on
22/07/24 17:59:40 INFO ParquetOutputFormat: Validation is off
22/07/24 17:59:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 17:59:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 17:59:40 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 17:59:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 17:59:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 17:59:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ss_sold_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_sold_time_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_store_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_promo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_ticket_number",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_quantity",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_wholesale_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_list_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_sales_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_ext_discount_amt",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_ext_sales_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_ext_wholesale_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_ext_list_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_ext_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_coupon_amt",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_net_paid",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_net_paid_inc_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ss_net_profit",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 ss_sold_date_sk;
  optional int32 ss_sold_time_sk;
  optional int32 ss_item_sk;
  optional int32 ss_customer_sk;
  optional int32 ss_cdemo_sk;
  optional int32 ss_hdemo_sk;
  optional int32 ss_addr_sk;
  optional int32 ss_store_sk;
  optional int32 ss_promo_sk;
  optional int32 ss_ticket_number;
  optional int32 ss_quantity;
  optional int32 ss_wholesale_cost (DECIMAL(7,2));
  optional int32 ss_list_price (DECIMAL(7,2));
  optional int32 ss_sales_price (DECIMAL(7,2));
  optional int32 ss_ext_discount_amt (DECIMAL(7,2));
  optional int32 ss_ext_sales_price (DECIMAL(7,2));
  optional int32 ss_ext_wholesale_cost (DECIMAL(7,2));
  optional int32 ss_ext_list_price (DECIMAL(7,2));
  optional int32 ss_ext_tax (DECIMAL(7,2));
  optional int32 ss_coupon_amt (DECIMAL(7,2));
  optional int32 ss_net_paid (DECIMAL(7,2));
  optional int32 ss_net_paid_inc_tax (DECIMAL(7,2));
  optional int32 ss_net_profit (DECIMAL(7,2));
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 17:59:40 INFO CodeGenerator: Code generated in 90.165856 ms
22/07/24 18:00:04 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 63.7 KiB, free: 366.2 MiB)
22/07/24 18:00:17 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 131748686
22/07/24 18:00:17 INFO FileOutputCommitter: Saved output of task 'attempt_20220724175940943329183762362309_0003_m_000000_3' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/store_sales/_temporary/0/task_20220724175940943329183762362309_0003_m_000000
22/07/24 18:00:17 INFO SparkHadoopMapRedUtil: attempt_20220724175940943329183762362309_0003_m_000000_3: Committed
22/07/24 18:00:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2396 bytes result sent to driver
22/07/24 18:00:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 37099 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:17 INFO DAGScheduler: ResultStage 3 (main at NativeMethodAccessorImpl.java:0) finished in 37.270 s
22/07/24 18:00:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
22/07/24 18:00:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
22/07/24 18:00:17 INFO DAGScheduler: Job 3 finished: main at NativeMethodAccessorImpl.java:0, took 37.273599 s
22/07/24 18:00:17 INFO FileFormatWriter: Write Job 88238a80-cfee-4cfd-87d9-6186aad7dafd committed.
22/07/24 18:00:17 INFO FileFormatWriter: Finished processing stats for write job 88238a80-cfee-4cfd-87d9-6186aad7dafd.
22/07/24 18:00:17 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:17 INFO CodeGenerator: Code generated in 6.524011 ms
22/07/24 18:00:17 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:17 INFO DAGScheduler: Got job 4 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:17 INFO DAGScheduler: Final stage: ResultStage 4 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:17 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:17 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:17 INFO DAGScheduler: Submitting ResultStage 4 (CoalescedRDD[42] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 211.9 KiB, free 365.8 MiB)
22/07/24 18:00:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 67.6 KiB, free 365.7 MiB)
22/07/24 18:00:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 67.6 KiB, free: 366.2 MiB)
22/07/24 18:00:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (CoalescedRDD[42] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:17 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
22/07/24 18:00:17 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:17 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
22/07/24 18:00:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:17 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:17 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:17 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:17 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:17 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:17 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:17 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:17 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:17 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:17 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:17 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:17 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:17 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:17 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:17 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:17 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "sr_returned_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_return_time_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_store_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_reason_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_ticket_number",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_return_quantity",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_return_amt",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_return_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_return_amt_inc_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_fee",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_return_ship_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_refunded_cash",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_reversed_charge",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_store_credit",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sr_net_loss",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 sr_returned_date_sk;
  optional int32 sr_return_time_sk;
  optional int32 sr_item_sk;
  optional int32 sr_customer_sk;
  optional int32 sr_cdemo_sk;
  optional int32 sr_hdemo_sk;
  optional int32 sr_addr_sk;
  optional int32 sr_store_sk;
  optional int32 sr_reason_sk;
  optional int32 sr_ticket_number;
  optional int32 sr_return_quantity;
  optional int32 sr_return_amt (DECIMAL(7,2));
  optional int32 sr_return_tax (DECIMAL(7,2));
  optional int32 sr_return_amt_inc_tax (DECIMAL(7,2));
  optional int32 sr_fee (DECIMAL(7,2));
  optional int32 sr_return_ship_cost (DECIMAL(7,2));
  optional int32 sr_refunded_cash (DECIMAL(7,2));
  optional int32 sr_reversed_charge (DECIMAL(7,2));
  optional int32 sr_store_credit (DECIMAL(7,2));
  optional int32 sr_net_loss (DECIMAL(7,2));
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:17 INFO CodeGenerator: Code generated in 22.483555 ms
22/07/24 18:00:25 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 15742407
22/07/24 18:00:25 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800171808505187639887769_0004_m_000000_4' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/store_returns/_temporary/0/task_202207241800171808505187639887769_0004_m_000000
22/07/24 18:00:25 INFO SparkHadoopMapRedUtil: attempt_202207241800171808505187639887769_0004_m_000000_4: Committed
22/07/24 18:00:25 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2396 bytes result sent to driver
22/07/24 18:00:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 7951 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
22/07/24 18:00:25 INFO DAGScheduler: ResultStage 4 (main at NativeMethodAccessorImpl.java:0) finished in 7.993 s
22/07/24 18:00:25 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
22/07/24 18:00:25 INFO DAGScheduler: Job 4 finished: main at NativeMethodAccessorImpl.java:0, took 7.995172 s
22/07/24 18:00:25 INFO FileFormatWriter: Write Job 5d86366d-8a94-4ec7-8973-b2e52140a057 committed.
22/07/24 18:00:25 INFO FileFormatWriter: Finished processing stats for write job 5d86366d-8a94-4ec7-8973-b2e52140a057.
22/07/24 18:00:25 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:25 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:25 INFO DAGScheduler: Got job 5 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:25 INFO DAGScheduler: Final stage: ResultStage 5 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:25 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:25 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:25 INFO DAGScheduler: Submitting ResultStage 5 (CoalescedRDD[51] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 234.0 KiB, free 365.5 MiB)
22/07/24 18:00:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 70.9 KiB, free 365.5 MiB)
22/07/24 18:00:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 70.9 KiB, free: 366.1 MiB)
22/07/24 18:00:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (CoalescedRDD[51] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:25 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
22/07/24 18:00:25 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:25 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
22/07/24 18:00:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:25 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:25 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:25 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:25 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:25 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:25 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:25 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:25 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:25 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:25 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:25 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:25 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:25 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:25 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ws_sold_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_sold_time_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ship_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_bill_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_bill_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_bill_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_bill_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ship_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ship_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ship_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ship_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_web_page_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_web_site_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ship_mode_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_warehouse_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_promo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_order_number",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_quantity",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_wholesale_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_list_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_sales_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ext_discount_amt",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ext_sales_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ext_wholesale_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ext_list_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ext_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_coupon_amt",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_ext_ship_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_net_paid",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_net_paid_inc_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_net_paid_inc_ship",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_net_paid_inc_ship_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ws_net_profit",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 ws_sold_date_sk;
  optional int32 ws_sold_time_sk;
  optional int32 ws_ship_date_sk;
  optional int32 ws_item_sk;
  optional int32 ws_bill_customer_sk;
  optional int32 ws_bill_cdemo_sk;
  optional int32 ws_bill_hdemo_sk;
  optional int32 ws_bill_addr_sk;
  optional int32 ws_ship_customer_sk;
  optional int32 ws_ship_cdemo_sk;
  optional int32 ws_ship_hdemo_sk;
  optional int32 ws_ship_addr_sk;
  optional int32 ws_web_page_sk;
  optional int32 ws_web_site_sk;
  optional int32 ws_ship_mode_sk;
  optional int32 ws_warehouse_sk;
  optional int32 ws_promo_sk;
  optional int32 ws_order_number;
  optional int32 ws_quantity;
  optional int32 ws_wholesale_cost (DECIMAL(7,2));
  optional int32 ws_list_price (DECIMAL(7,2));
  optional int32 ws_sales_price (DECIMAL(7,2));
  optional int32 ws_ext_discount_amt (DECIMAL(7,2));
  optional int32 ws_ext_sales_price (DECIMAL(7,2));
  optional int32 ws_ext_wholesale_cost (DECIMAL(7,2));
  optional int32 ws_ext_list_price (DECIMAL(7,2));
  optional int32 ws_ext_tax (DECIMAL(7,2));
  optional int32 ws_coupon_amt (DECIMAL(7,2));
  optional int32 ws_ext_ship_cost (DECIMAL(7,2));
  optional int32 ws_net_paid (DECIMAL(7,2));
  optional int32 ws_net_paid_inc_tax (DECIMAL(7,2));
  optional int32 ws_net_paid_inc_ship (DECIMAL(7,2));
  optional int32 ws_net_paid_inc_ship_tax (DECIMAL(7,2));
  optional int32 ws_net_profit (DECIMAL(7,2));
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:28 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 67.6 KiB, free: 366.2 MiB)
22/07/24 18:00:28 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 68.5 KiB, free: 366.2 MiB)
22/07/24 18:00:35 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 61179989
22/07/24 18:00:35 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800252679148624694231441_0005_m_000000_5' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/web_sales/_temporary/0/task_202207241800252679148624694231441_0005_m_000000
22/07/24 18:00:35 INFO SparkHadoopMapRedUtil: attempt_202207241800252679148624694231441_0005_m_000000_5: Committed
22/07/24 18:00:35 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2396 bytes result sent to driver
22/07/24 18:00:35 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 10235 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:35 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
22/07/24 18:00:35 INFO DAGScheduler: ResultStage 5 (main at NativeMethodAccessorImpl.java:0) finished in 10.272 s
22/07/24 18:00:35 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
22/07/24 18:00:35 INFO DAGScheduler: Job 5 finished: main at NativeMethodAccessorImpl.java:0, took 10.275040 s
22/07/24 18:00:35 INFO FileFormatWriter: Write Job 9ab4fe16-ff52-4855-8323-08b312e24ba5 committed.
22/07/24 18:00:35 INFO FileFormatWriter: Finished processing stats for write job 9ab4fe16-ff52-4855-8323-08b312e24ba5.
22/07/24 18:00:35 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:35 INFO CodeGenerator: Code generated in 8.868111 ms
22/07/24 18:00:35 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:35 INFO DAGScheduler: Got job 6 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:35 INFO DAGScheduler: Final stage: ResultStage 6 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:35 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:35 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:35 INFO DAGScheduler: Submitting ResultStage 6 (CoalescedRDD[60] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 218.3 KiB, free 365.8 MiB)
22/07/24 18:00:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 68.6 KiB, free 365.7 MiB)
22/07/24 18:00:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 68.6 KiB, free: 366.2 MiB)
22/07/24 18:00:35 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (CoalescedRDD[60] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
22/07/24 18:00:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
22/07/24 18:00:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:35 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:35 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:35 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:35 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:35 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:35 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:35 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:35 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:35 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:35 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:35 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:35 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:35 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "wr_returned_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_returned_time_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_refunded_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_refunded_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_refunded_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_refunded_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_returning_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_returning_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_returning_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_returning_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_web_page_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_reason_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_order_number",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_return_quantity",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_return_amt",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_return_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_return_amt_inc_tax",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_fee",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_return_ship_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_refunded_cash",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_reversed_charge",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_account_credit",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wr_net_loss",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 wr_returned_date_sk;
  optional int32 wr_returned_time_sk;
  optional int32 wr_item_sk;
  optional int32 wr_refunded_customer_sk;
  optional int32 wr_refunded_cdemo_sk;
  optional int32 wr_refunded_hdemo_sk;
  optional int32 wr_refunded_addr_sk;
  optional int32 wr_returning_customer_sk;
  optional int32 wr_returning_cdemo_sk;
  optional int32 wr_returning_hdemo_sk;
  optional int32 wr_returning_addr_sk;
  optional int32 wr_web_page_sk;
  optional int32 wr_reason_sk;
  optional int32 wr_order_number;
  optional int32 wr_return_quantity;
  optional int32 wr_return_amt (DECIMAL(7,2));
  optional int32 wr_return_tax (DECIMAL(7,2));
  optional int32 wr_return_amt_inc_tax (DECIMAL(7,2));
  optional int32 wr_fee (DECIMAL(7,2));
  optional int32 wr_return_ship_cost (DECIMAL(7,2));
  optional int32 wr_refunded_cash (DECIMAL(7,2));
  optional int32 wr_reversed_charge (DECIMAL(7,2));
  optional int32 wr_account_credit (DECIMAL(7,2));
  optional int32 wr_net_loss (DECIMAL(7,2));
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:36 INFO CodeGenerator: Code generated in 26.237566 ms
22/07/24 18:00:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9854552
22/07/24 18:00:38 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800357067361171626247559_0006_m_000000_6' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/web_returns/_temporary/0/task_202207241800357067361171626247559_0006_m_000000
22/07/24 18:00:38 INFO SparkHadoopMapRedUtil: attempt_202207241800357067361171626247559_0006_m_000000_6: Committed
22/07/24 18:00:38 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2396 bytes result sent to driver
22/07/24 18:00:38 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 2421 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:38 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
22/07/24 18:00:38 INFO DAGScheduler: ResultStage 6 (main at NativeMethodAccessorImpl.java:0) finished in 2.433 s
22/07/24 18:00:38 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
22/07/24 18:00:38 INFO DAGScheduler: Job 6 finished: main at NativeMethodAccessorImpl.java:0, took 2.435111 s
22/07/24 18:00:38 INFO FileFormatWriter: Write Job daa56b60-3024-4fc0-afae-37182e084bd0 committed.
22/07/24 18:00:38 INFO FileFormatWriter: Finished processing stats for write job daa56b60-3024-4fc0-afae-37182e084bd0.
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO CodeGenerator: Code generated in 8.704537 ms
22/07/24 18:00:38 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:38 INFO DAGScheduler: Got job 7 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:38 INFO DAGScheduler: Final stage: ResultStage 7 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:38 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:38 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:38 INFO DAGScheduler: Submitting ResultStage 7 (CoalescedRDD[69] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:38 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 224.7 KiB, free 365.5 MiB)
22/07/24 18:00:38 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 69.6 KiB, free 365.4 MiB)
22/07/24 18:00:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 69.6 KiB, free: 366.1 MiB)
22/07/24 18:00:38 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (CoalescedRDD[69] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:38 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
22/07/24 18:00:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:38 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
22/07/24 18:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:38 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:38 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:38 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:38 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:38 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:38 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "cc_call_center_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_call_center_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_rec_start_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_rec_end_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_closed_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_open_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_class",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_employees",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_sq_ft",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_hours",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_manager",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_mkt_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_mkt_class",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_mkt_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_market_manager",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_division",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_division_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_company",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_company_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_street_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_street_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_street_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_suite_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_county",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_state",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_zip",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_gmt_offset",
    "type" : "decimal(5,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cc_tax_percentage",
    "type" : "decimal(5,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 cc_call_center_sk;
  optional binary cc_call_center_id (UTF8);
  optional int32 cc_rec_start_date (DATE);
  optional int32 cc_rec_end_date (DATE);
  optional int32 cc_closed_date_sk;
  optional int32 cc_open_date_sk;
  optional binary cc_name (UTF8);
  optional binary cc_class (UTF8);
  optional int32 cc_employees;
  optional int32 cc_sq_ft;
  optional binary cc_hours (UTF8);
  optional binary cc_manager (UTF8);
  optional int32 cc_mkt_id;
  optional binary cc_mkt_class (UTF8);
  optional binary cc_mkt_desc (UTF8);
  optional binary cc_market_manager (UTF8);
  optional int32 cc_division;
  optional binary cc_division_name (UTF8);
  optional int32 cc_company;
  optional binary cc_company_name (UTF8);
  optional binary cc_street_number (UTF8);
  optional binary cc_street_name (UTF8);
  optional binary cc_street_type (UTF8);
  optional binary cc_suite_number (UTF8);
  optional binary cc_city (UTF8);
  optional binary cc_county (UTF8);
  optional binary cc_state (UTF8);
  optional binary cc_zip (UTF8);
  optional binary cc_country (UTF8);
  optional int32 cc_gmt_offset (DECIMAL(5,2));
  optional int32 cc_tax_percentage (DECIMAL(5,2));
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:38 INFO CodeGenerator: Code generated in 33.251877 ms
22/07/24 18:00:38 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 2238
22/07/24 18:00:38 INFO FileOutputCommitter: Saved output of task 'attempt_20220724180038488368057500210255_0007_m_000000_7' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/call_center/_temporary/0/task_20220724180038488368057500210255_0007_m_000000
22/07/24 18:00:38 INFO SparkHadoopMapRedUtil: attempt_20220724180038488368057500210255_0007_m_000000_7: Committed
22/07/24 18:00:38 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2396 bytes result sent to driver
22/07/24 18:00:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 150 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:38 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
22/07/24 18:00:38 INFO DAGScheduler: ResultStage 7 (main at NativeMethodAccessorImpl.java:0) finished in 0.166 s
22/07/24 18:00:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
22/07/24 18:00:38 INFO DAGScheduler: Job 7 finished: main at NativeMethodAccessorImpl.java:0, took 0.168736 s
22/07/24 18:00:38 INFO FileFormatWriter: Write Job f0480a38-25d5-4677-bb58-2a09fb6a38a6 committed.
22/07/24 18:00:38 INFO FileFormatWriter: Finished processing stats for write job f0480a38-25d5-4677-bb58-2a09fb6a38a6.
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO CodeGenerator: Code generated in 6.324831 ms
22/07/24 18:00:38 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:38 INFO DAGScheduler: Got job 8 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:38 INFO DAGScheduler: Final stage: ResultStage 8 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:38 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:38 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:38 INFO DAGScheduler: Submitting ResultStage 8 (CoalescedRDD[78] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:38 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 193.5 KiB, free 365.2 MiB)
22/07/24 18:00:38 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 64.8 KiB, free 365.2 MiB)
22/07/24 18:00:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 64.8 KiB, free: 366.0 MiB)
22/07/24 18:00:38 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (CoalescedRDD[78] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:38 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
22/07/24 18:00:38 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:38 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
22/07/24 18:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:38 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:38 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:38 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:38 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:38 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:38 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:38 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:38 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:38 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:38 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:38 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:38 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "cp_catalog_page_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cp_catalog_page_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cp_start_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cp_end_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cp_department",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cp_catalog_number",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cp_catalog_page_number",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cp_description",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cp_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 cp_catalog_page_sk;
  optional binary cp_catalog_page_id (UTF8);
  optional int32 cp_start_date_sk;
  optional int32 cp_end_date_sk;
  optional binary cp_department (UTF8);
  optional int32 cp_catalog_number;
  optional int32 cp_catalog_page_number;
  optional binary cp_description (UTF8);
  optional binary cp_type (UTF8);
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:38 INFO CodeGenerator: Code generated in 8.398973 ms
22/07/24 18:00:39 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 1617237
22/07/24 18:00:39 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800387778926330573726053_0008_m_000000_8' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/catalog_page/_temporary/0/task_202207241800387778926330573726053_0008_m_000000
22/07/24 18:00:39 INFO SparkHadoopMapRedUtil: attempt_202207241800387778926330573726053_0008_m_000000_8: Committed
22/07/24 18:00:39 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2353 bytes result sent to driver
22/07/24 18:00:39 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 182 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:39 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
22/07/24 18:00:39 INFO DAGScheduler: ResultStage 8 (main at NativeMethodAccessorImpl.java:0) finished in 0.200 s
22/07/24 18:00:39 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
22/07/24 18:00:39 INFO DAGScheduler: Job 8 finished: main at NativeMethodAccessorImpl.java:0, took 0.202601 s
22/07/24 18:00:39 INFO FileFormatWriter: Write Job 2d27e602-a10a-4ec4-9f41-67ba8568174c committed.
22/07/24 18:00:39 INFO FileFormatWriter: Finished processing stats for write job 2d27e602-a10a-4ec4-9f41-67ba8568174c.
22/07/24 18:00:39 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:39 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:39 INFO CodeGenerator: Code generated in 5.338964 ms
22/07/24 18:00:39 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:39 INFO DAGScheduler: Got job 9 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:39 INFO DAGScheduler: Final stage: ResultStage 9 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:39 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:39 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:39 INFO DAGScheduler: Submitting ResultStage 9 (CoalescedRDD[87] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:39 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 207.7 KiB, free 365.0 MiB)
22/07/24 18:00:39 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 66.5 KiB, free 364.9 MiB)
22/07/24 18:00:39 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 66.5 KiB, free: 366.0 MiB)
22/07/24 18:00:39 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (CoalescedRDD[87] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
22/07/24 18:00:39 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:39 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
22/07/24 18:00:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:39 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:39 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:39 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:39 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:39 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:39 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:39 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:39 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:39 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:39 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:39 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:39 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:39 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "c_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_customer_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_current_cdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_current_hdemo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_current_addr_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_first_shipto_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_first_sales_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_salutation",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_first_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_last_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_preferred_cust_flag",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_birth_day",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_birth_month",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_birth_year",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_birth_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_login",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_email_address",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "c_last_review_date",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 c_customer_sk;
  optional binary c_customer_id (UTF8);
  optional int32 c_current_cdemo_sk;
  optional int32 c_current_hdemo_sk;
  optional int32 c_current_addr_sk;
  optional int32 c_first_shipto_date_sk;
  optional int32 c_first_sales_date_sk;
  optional binary c_salutation (UTF8);
  optional binary c_first_name (UTF8);
  optional binary c_last_name (UTF8);
  optional binary c_preferred_cust_flag (UTF8);
  optional int32 c_birth_day;
  optional int32 c_birth_month;
  optional int32 c_birth_year;
  optional binary c_birth_country (UTF8);
  optional binary c_login (UTF8);
  optional binary c_email_address (UTF8);
  optional int32 c_last_review_date;
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:39 INFO CodeGenerator: Code generated in 19.68024 ms
22/07/24 18:00:39 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 70.9 KiB, free: 366.0 MiB)
22/07/24 18:00:39 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 64.8 KiB, free: 366.1 MiB)
22/07/24 18:00:39 INFO BlockManagerInfo: Removed broadcast_7_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 69.6 KiB, free: 366.2 MiB)
22/07/24 18:00:39 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 68.6 KiB, free: 366.2 MiB)
22/07/24 18:00:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 10864080
22/07/24 18:00:40 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800398272046535761249146_0009_m_000000_9' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/customer/_temporary/0/task_202207241800398272046535761249146_0009_m_000000
22/07/24 18:00:40 INFO SparkHadoopMapRedUtil: attempt_202207241800398272046535761249146_0009_m_000000_9: Committed
22/07/24 18:00:40 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2396 bytes result sent to driver
22/07/24 18:00:40 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 1319 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:40 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
22/07/24 18:00:40 INFO DAGScheduler: ResultStage 9 (main at NativeMethodAccessorImpl.java:0) finished in 1.329 s
22/07/24 18:00:40 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
22/07/24 18:00:40 INFO DAGScheduler: Job 9 finished: main at NativeMethodAccessorImpl.java:0, took 1.331005 s
22/07/24 18:00:40 INFO FileFormatWriter: Write Job 18cf03e4-a068-40ae-9d60-f260b4ea91e0 committed.
22/07/24 18:00:40 INFO FileFormatWriter: Finished processing stats for write job 18cf03e4-a068-40ae-9d60-f260b4ea91e0.
22/07/24 18:00:40 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:40 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:40 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:40 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:40 INFO CodeGenerator: Code generated in 7.008129 ms
22/07/24 18:00:40 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:40 INFO DAGScheduler: Got job 10 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:40 INFO DAGScheduler: Final stage: ResultStage 10 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:40 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:40 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:40 INFO DAGScheduler: Submitting ResultStage 10 (CoalescedRDD[96] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:40 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 198.9 KiB, free 365.8 MiB)
22/07/24 18:00:40 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 65.7 KiB, free 365.8 MiB)
22/07/24 18:00:40 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 65.7 KiB, free: 366.2 MiB)
22/07/24 18:00:40 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (CoalescedRDD[96] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:40 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
22/07/24 18:00:40 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:40 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
22/07/24 18:00:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:40 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:40 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:40 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:40 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:40 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:40 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:40 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:40 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:40 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:40 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:40 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:40 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ca_address_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_address_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_street_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_street_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_street_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_suite_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_county",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_state",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_zip",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_gmt_offset",
    "type" : "decimal(5,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ca_location_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 ca_address_sk;
  optional binary ca_address_id (UTF8);
  optional binary ca_street_number (UTF8);
  optional binary ca_street_name (UTF8);
  optional binary ca_street_type (UTF8);
  optional binary ca_suite_number (UTF8);
  optional binary ca_city (UTF8);
  optional binary ca_county (UTF8);
  optional binary ca_state (UTF8);
  optional binary ca_zip (UTF8);
  optional binary ca_country (UTF8);
  optional int32 ca_gmt_offset (DECIMAL(5,2));
  optional binary ca_location_type (UTF8);
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:40 INFO CodeGenerator: Code generated in 13.43815 ms
22/07/24 18:00:40 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 3874344
22/07/24 18:00:40 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800406851782104763993370_0010_m_000000_10' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/customer_address/_temporary/0/task_202207241800406851782104763993370_0010_m_000000
22/07/24 18:00:40 INFO SparkHadoopMapRedUtil: attempt_202207241800406851782104763993370_0010_m_000000_10: Committed
22/07/24 18:00:40 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2396 bytes result sent to driver
22/07/24 18:00:40 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 439 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:40 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
22/07/24 18:00:40 INFO DAGScheduler: ResultStage 10 (main at NativeMethodAccessorImpl.java:0) finished in 0.451 s
22/07/24 18:00:40 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
22/07/24 18:00:40 INFO DAGScheduler: Job 10 finished: main at NativeMethodAccessorImpl.java:0, took 0.452668 s
22/07/24 18:00:40 INFO FileFormatWriter: Write Job 8323b3ff-6df8-4a5f-a100-e958b79cfc5a committed.
22/07/24 18:00:40 INFO FileFormatWriter: Finished processing stats for write job 8323b3ff-6df8-4a5f-a100-e958b79cfc5a.
22/07/24 18:00:40 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:40 INFO CodeGenerator: Code generated in 3.769867 ms
22/07/24 18:00:41 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:41 INFO DAGScheduler: Got job 11 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:41 INFO DAGScheduler: Final stage: ResultStage 11 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:41 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:41 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:41 INFO DAGScheduler: Submitting ResultStage 11 (CoalescedRDD[105] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:41 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 194.4 KiB, free 365.6 MiB)
22/07/24 18:00:41 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 65.2 KiB, free 365.5 MiB)
22/07/24 18:00:41 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 65.2 KiB, free: 366.1 MiB)
22/07/24 18:00:41 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (CoalescedRDD[105] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:41 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
22/07/24 18:00:41 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:41 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
22/07/24 18:00:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:41 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:41 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:41 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:41 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:41 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:41 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:41 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:41 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:41 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:41 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:41 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:41 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:41 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "cd_demo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cd_gender",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cd_marital_status",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cd_education_status",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cd_purchase_estimate",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cd_credit_rating",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cd_dep_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cd_dep_employed_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "cd_dep_college_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 cd_demo_sk;
  optional binary cd_gender (UTF8);
  optional binary cd_marital_status (UTF8);
  optional binary cd_education_status (UTF8);
  optional int32 cd_purchase_estimate;
  optional binary cd_credit_rating (UTF8);
  optional int32 cd_dep_count;
  optional int32 cd_dep_employed_count;
  optional int32 cd_dep_college_count;
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:44 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9928540
22/07/24 18:00:44 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800412671082796981297116_0011_m_000000_11' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/customer_demographics/_temporary/0/task_202207241800412671082796981297116_0011_m_000000
22/07/24 18:00:44 INFO SparkHadoopMapRedUtil: attempt_202207241800412671082796981297116_0011_m_000000_11: Committed
22/07/24 18:00:44 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 2396 bytes result sent to driver
22/07/24 18:00:44 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 3565 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:44 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
22/07/24 18:00:44 INFO DAGScheduler: ResultStage 11 (main at NativeMethodAccessorImpl.java:0) finished in 3.581 s
22/07/24 18:00:44 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
22/07/24 18:00:44 INFO DAGScheduler: Job 11 finished: main at NativeMethodAccessorImpl.java:0, took 3.584535 s
22/07/24 18:00:44 INFO FileFormatWriter: Write Job 377824c1-402b-4633-89ba-2b8d1ecab4d4 committed.
22/07/24 18:00:44 INFO FileFormatWriter: Finished processing stats for write job 377824c1-402b-4633-89ba-2b8d1ecab4d4.
22/07/24 18:00:44 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:44 INFO CodeGenerator: Code generated in 15.925983 ms
22/07/24 18:00:44 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:44 INFO DAGScheduler: Got job 12 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:44 INFO DAGScheduler: Final stage: ResultStage 12 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:44 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:44 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:44 INFO DAGScheduler: Submitting ResultStage 12 (CoalescedRDD[114] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:44 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 223.0 KiB, free 365.3 MiB)
22/07/24 18:00:44 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 69.1 KiB, free 365.2 MiB)
22/07/24 18:00:44 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 69.1 KiB, free: 366.0 MiB)
22/07/24 18:00:44 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (CoalescedRDD[114] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:44 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
22/07/24 18:00:44 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:44 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
22/07/24 18:00:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:44 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:44 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:44 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:44 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:44 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:44 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:44 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:44 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:44 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:44 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:44 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:44 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:44 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:44 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "d_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_date_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_month_seq",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_week_seq",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_quarter_seq",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_year",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_dow",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_moy",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_dom",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_qoy",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_fy_year",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_fy_quarter_seq",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_fy_week_seq",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_day_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_quarter_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_holiday",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_weekend",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_following_holiday",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_first_dom",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_last_dom",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_same_day_ly",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_same_day_lq",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_current_day",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_current_week",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_current_month",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_current_quarter",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "d_current_year",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 d_date_sk;
  optional binary d_date_id (UTF8);
  optional int32 d_date (DATE);
  optional int32 d_month_seq;
  optional int32 d_week_seq;
  optional int32 d_quarter_seq;
  optional int32 d_year;
  optional int32 d_dow;
  optional int32 d_moy;
  optional int32 d_dom;
  optional int32 d_qoy;
  optional int32 d_fy_year;
  optional int32 d_fy_quarter_seq;
  optional int32 d_fy_week_seq;
  optional binary d_day_name (UTF8);
  optional binary d_quarter_name (UTF8);
  optional binary d_holiday (UTF8);
  optional binary d_weekend (UTF8);
  optional binary d_following_holiday (UTF8);
  optional int32 d_first_dom;
  optional int32 d_last_dom;
  optional int32 d_same_day_ly;
  optional int32 d_same_day_lq;
  optional binary d_current_day (UTF8);
  optional binary d_current_week (UTF8);
  optional binary d_current_month (UTF8);
  optional binary d_current_quarter (UTF8);
  optional binary d_current_year (UTF8);
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:44 INFO CodeGenerator: Code generated in 26.936185 ms
22/07/24 18:00:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 9955302
22/07/24 18:00:45 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800441870431334512756462_0012_m_000000_12' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/date_dim/_temporary/0/task_202207241800441870431334512756462_0012_m_000000
22/07/24 18:00:45 INFO SparkHadoopMapRedUtil: attempt_202207241800441870431334512756462_0012_m_000000_12: Committed
22/07/24 18:00:45 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 2396 bytes result sent to driver
22/07/24 18:00:45 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 687 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:45 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
22/07/24 18:00:45 INFO DAGScheduler: ResultStage 12 (main at NativeMethodAccessorImpl.java:0) finished in 0.699 s
22/07/24 18:00:45 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
22/07/24 18:00:45 INFO DAGScheduler: Job 12 finished: main at NativeMethodAccessorImpl.java:0, took 0.700941 s
22/07/24 18:00:45 INFO FileFormatWriter: Write Job 456e2631-3dcd-417d-b221-0f61f2222522 committed.
22/07/24 18:00:45 INFO FileFormatWriter: Finished processing stats for write job 456e2631-3dcd-417d-b221-0f61f2222522.
22/07/24 18:00:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO CodeGenerator: Code generated in 3.557333 ms
22/07/24 18:00:45 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:45 INFO DAGScheduler: Got job 13 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:45 INFO DAGScheduler: Final stage: ResultStage 13 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:45 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:45 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:45 INFO DAGScheduler: Submitting ResultStage 13 (CoalescedRDD[123] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:45 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 188.4 KiB, free 365.1 MiB)
22/07/24 18:00:45 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 64.2 KiB, free 365.0 MiB)
22/07/24 18:00:45 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 64.2 KiB, free: 366.0 MiB)
22/07/24 18:00:45 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (CoalescedRDD[123] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:45 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
22/07/24 18:00:45 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:45 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:45 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:45 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:45 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:45 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "hd_demo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "hd_income_band_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "hd_buy_potential",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "hd_dep_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "hd_vehicle_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 hd_demo_sk;
  optional int32 hd_income_band_sk;
  optional binary hd_buy_potential (UTF8);
  optional int32 hd_dep_count;
  optional int32 hd_vehicle_count;
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:45 INFO CodeGenerator: Code generated in 7.73717 ms
22/07/24 18:00:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 173058
22/07/24 18:00:45 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800453183132444581140386_0013_m_000000_13' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/household_demographics/_temporary/0/task_202207241800453183132444581140386_0013_m_000000
22/07/24 18:00:45 INFO SparkHadoopMapRedUtil: attempt_202207241800453183132444581140386_0013_m_000000_13: Committed
22/07/24 18:00:45 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 2396 bytes result sent to driver
22/07/24 18:00:45 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 60 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:45 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
22/07/24 18:00:45 INFO DAGScheduler: ResultStage 13 (main at NativeMethodAccessorImpl.java:0) finished in 0.069 s
22/07/24 18:00:45 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
22/07/24 18:00:45 INFO DAGScheduler: Job 13 finished: main at NativeMethodAccessorImpl.java:0, took 0.070531 s
22/07/24 18:00:45 INFO FileFormatWriter: Write Job eb451e61-f455-4ca4-8610-df45d772aa00 committed.
22/07/24 18:00:45 INFO FileFormatWriter: Finished processing stats for write job eb451e61-f455-4ca4-8610-df45d772aa00.
22/07/24 18:00:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO CodeGenerator: Code generated in 4.336484 ms
22/07/24 18:00:45 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:45 INFO DAGScheduler: Got job 14 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:45 INFO DAGScheduler: Final stage: ResultStage 14 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:45 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:45 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:45 INFO DAGScheduler: Submitting ResultStage 14 (CoalescedRDD[132] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:45 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 185.4 KiB, free 364.8 MiB)
22/07/24 18:00:45 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 63.7 KiB, free 364.7 MiB)
22/07/24 18:00:45 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 63.7 KiB, free: 365.9 MiB)
22/07/24 18:00:45 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (CoalescedRDD[132] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:45 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
22/07/24 18:00:45 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:45 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:45 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:45 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:45 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:45 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ib_income_band_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ib_lower_bound",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ib_upper_bound",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 ib_income_band_sk;
  optional int32 ib_lower_bound;
  optional int32 ib_upper_bound;
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:45 INFO CodeGenerator: Code generated in 5.70195 ms
22/07/24 18:00:45 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 480
22/07/24 18:00:45 INFO FileOutputCommitter: Saved output of task 'attempt_20220724180045944057737525549963_0014_m_000000_14' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/income_band/_temporary/0/task_20220724180045944057737525549963_0014_m_000000
22/07/24 18:00:45 INFO SparkHadoopMapRedUtil: attempt_20220724180045944057737525549963_0014_m_000000_14: Committed
22/07/24 18:00:45 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 2353 bytes result sent to driver
22/07/24 18:00:45 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 27 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:45 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
22/07/24 18:00:45 INFO DAGScheduler: ResultStage 14 (main at NativeMethodAccessorImpl.java:0) finished in 0.045 s
22/07/24 18:00:45 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
22/07/24 18:00:45 INFO DAGScheduler: Job 14 finished: main at NativeMethodAccessorImpl.java:0, took 0.047022 s
22/07/24 18:00:45 INFO FileFormatWriter: Write Job 9f72d28b-2fb0-48f7-887a-eb113e385f84 committed.
22/07/24 18:00:45 INFO FileFormatWriter: Finished processing stats for write job 9f72d28b-2fb0-48f7-887a-eb113e385f84.
22/07/24 18:00:45 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:45 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO CodeGenerator: Code generated in 8.879367 ms
22/07/24 18:00:45 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:45 INFO DAGScheduler: Got job 15 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:45 INFO DAGScheduler: Final stage: ResultStage 15 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:45 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:45 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:45 INFO DAGScheduler: Submitting ResultStage 15 (CoalescedRDD[141] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:45 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 213.6 KiB, free 364.5 MiB)
22/07/24 18:00:45 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 68.0 KiB, free 364.5 MiB)
22/07/24 18:00:45 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 68.0 KiB, free: 365.8 MiB)
22/07/24 18:00:45 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (CoalescedRDD[141] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:45 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
22/07/24 18:00:45 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:45 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:45 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:45 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:45 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:45 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:45 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:45 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:45 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:45 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:45 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:45 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:45 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:45 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "i_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_item_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_rec_start_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_rec_end_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_item_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_current_price",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_wholesale_cost",
    "type" : "decimal(7,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_brand_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_brand",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_class_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_class",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_category_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_category",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_manufact_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_manufact",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_size",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_formulation",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_color",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_units",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_container",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_manager_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "i_product_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 i_item_sk;
  optional binary i_item_id (UTF8);
  optional int32 i_rec_start_date (DATE);
  optional int32 i_rec_end_date (DATE);
  optional binary i_item_desc (UTF8);
  optional int32 i_current_price (DECIMAL(7,2));
  optional int32 i_wholesale_cost (DECIMAL(7,2));
  optional int32 i_brand_id;
  optional binary i_brand (UTF8);
  optional int32 i_class_id;
  optional binary i_class (UTF8);
  optional int32 i_category_id;
  optional binary i_category (UTF8);
  optional int32 i_manufact_id;
  optional binary i_manufact (UTF8);
  optional binary i_size (UTF8);
  optional binary i_formulation (UTF8);
  optional binary i_color (UTF8);
  optional binary i_units (UTF8);
  optional binary i_container (UTF8);
  optional int32 i_manager_id;
  optional binary i_product_name (UTF8);
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:45 INFO CodeGenerator: Code generated in 19.245417 ms
22/07/24 18:00:45 INFO BlockManagerInfo: Removed broadcast_14_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 63.7 KiB, free: 365.9 MiB)
22/07/24 18:00:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 4063302
22/07/24 18:00:46 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800454361530920469818471_0015_m_000000_15' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/item/_temporary/0/task_202207241800454361530920469818471_0015_m_000000
22/07/24 18:00:46 INFO SparkHadoopMapRedUtil: attempt_202207241800454361530920469818471_0015_m_000000_15: Committed
22/07/24 18:00:46 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 2396 bytes result sent to driver
22/07/24 18:00:46 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 352 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:46 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
22/07/24 18:00:46 INFO DAGScheduler: ResultStage 15 (main at NativeMethodAccessorImpl.java:0) finished in 0.368 s
22/07/24 18:00:46 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
22/07/24 18:00:46 INFO DAGScheduler: Job 15 finished: main at NativeMethodAccessorImpl.java:0, took 0.369585 s
22/07/24 18:00:46 INFO FileFormatWriter: Write Job efd1d55d-9bab-43aa-a991-6c4409de47f5 committed.
22/07/24 18:00:46 INFO FileFormatWriter: Finished processing stats for write job efd1d55d-9bab-43aa-a991-6c4409de47f5.
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 4.490562 ms
22/07/24 18:00:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:46 INFO DAGScheduler: Got job 16 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:46 INFO DAGScheduler: Final stage: ResultStage 16 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:46 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:46 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:46 INFO DAGScheduler: Submitting ResultStage 16 (CoalescedRDD[150] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 208.8 KiB, free 364.5 MiB)
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 66.6 KiB, free 364.4 MiB)
22/07/24 18:00:46 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 66.6 KiB, free: 365.8 MiB)
22/07/24 18:00:46 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (CoalescedRDD[150] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:46 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
22/07/24 18:00:46 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:46 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:46 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:46 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "p_promo_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_promo_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_start_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_end_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_item_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_cost",
    "type" : "decimal(15,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_response_target",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_promo_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_dmail",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_email",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_catalog",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_tv",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_radio",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_press",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_event",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_demo",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_channel_details",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_purpose",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "p_discount_active",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 p_promo_sk;
  optional binary p_promo_id (UTF8);
  optional int32 p_start_date_sk;
  optional int32 p_end_date_sk;
  optional int32 p_item_sk;
  optional int64 p_cost (DECIMAL(15,2));
  optional int32 p_response_target;
  optional binary p_promo_name (UTF8);
  optional binary p_channel_dmail (UTF8);
  optional binary p_channel_email (UTF8);
  optional binary p_channel_catalog (UTF8);
  optional binary p_channel_tv (UTF8);
  optional binary p_channel_radio (UTF8);
  optional binary p_channel_press (UTF8);
  optional binary p_channel_event (UTF8);
  optional binary p_channel_demo (UTF8);
  optional binary p_channel_details (UTF8);
  optional binary p_purpose (UTF8);
  optional binary p_discount_active (UTF8);
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 12.571169 ms
22/07/24 18:00:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 47702
22/07/24 18:00:46 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800462137277590681184487_0016_m_000000_16' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/promotion/_temporary/0/task_202207241800462137277590681184487_0016_m_000000
22/07/24 18:00:46 INFO SparkHadoopMapRedUtil: attempt_202207241800462137277590681184487_0016_m_000000_16: Committed
22/07/24 18:00:46 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 2353 bytes result sent to driver
22/07/24 18:00:46 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 54 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:46 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
22/07/24 18:00:46 INFO DAGScheduler: ResultStage 16 (main at NativeMethodAccessorImpl.java:0) finished in 0.064 s
22/07/24 18:00:46 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
22/07/24 18:00:46 INFO DAGScheduler: Job 16 finished: main at NativeMethodAccessorImpl.java:0, took 0.065836 s
22/07/24 18:00:46 INFO FileFormatWriter: Write Job ed6b6a28-7685-40e9-b0aa-12e8abe3b97d committed.
22/07/24 18:00:46 INFO FileFormatWriter: Finished processing stats for write job ed6b6a28-7685-40e9-b0aa-12e8abe3b97d.
22/07/24 18:00:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 3.438837 ms
22/07/24 18:00:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:46 INFO DAGScheduler: Got job 17 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:46 INFO DAGScheduler: Final stage: ResultStage 17 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:46 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:46 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:46 INFO DAGScheduler: Submitting ResultStage 17 (CoalescedRDD[159] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 185.2 KiB, free 364.3 MiB)
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 63.7 KiB, free 364.2 MiB)
22/07/24 18:00:46 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 63.7 KiB, free: 365.8 MiB)
22/07/24 18:00:46 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (CoalescedRDD[159] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:46 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
22/07/24 18:00:46 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:46 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:46 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:46 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "r_reason_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "r_reason_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "r_reason_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 r_reason_sk;
  optional binary r_reason_id (UTF8);
  optional binary r_reason_desc (UTF8);
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 4796
22/07/24 18:00:46 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800464001596724418442683_0017_m_000000_17' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/reason/_temporary/0/task_202207241800464001596724418442683_0017_m_000000
22/07/24 18:00:46 INFO SparkHadoopMapRedUtil: attempt_202207241800464001596724418442683_0017_m_000000_17: Committed
22/07/24 18:00:46 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 2353 bytes result sent to driver
22/07/24 18:00:46 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 21 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:46 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
22/07/24 18:00:46 INFO DAGScheduler: ResultStage 17 (main at NativeMethodAccessorImpl.java:0) finished in 0.031 s
22/07/24 18:00:46 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
22/07/24 18:00:46 INFO DAGScheduler: Job 17 finished: main at NativeMethodAccessorImpl.java:0, took 0.031942 s
22/07/24 18:00:46 INFO FileFormatWriter: Write Job ce199d71-0b06-4f39-8e24-6a1bb59a7f48 committed.
22/07/24 18:00:46 INFO FileFormatWriter: Finished processing stats for write job ce199d71-0b06-4f39-8e24-6a1bb59a7f48.
22/07/24 18:00:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 3.164616 ms
22/07/24 18:00:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:46 INFO DAGScheduler: Got job 18 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:46 INFO DAGScheduler: Final stage: ResultStage 18 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:46 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:46 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:46 INFO DAGScheduler: Submitting ResultStage 18 (CoalescedRDD[168] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 189.6 KiB, free 364.0 MiB)
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 64.0 KiB, free 364.0 MiB)
22/07/24 18:00:46 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 64.0 KiB, free: 365.7 MiB)
22/07/24 18:00:46 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (CoalescedRDD[168] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:46 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
22/07/24 18:00:46 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:46 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:46 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:46 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "sm_ship_mode_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sm_ship_mode_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sm_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sm_code",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sm_carrier",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sm_contract",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 sm_ship_mode_sk;
  optional binary sm_ship_mode_id (UTF8);
  optional binary sm_type (UTF8);
  optional binary sm_code (UTF8);
  optional binary sm_carrier (UTF8);
  optional binary sm_contract (UTF8);
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 7.644916 ms
22/07/24 18:00:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 2180
22/07/24 18:00:46 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800462157304366773448997_0018_m_000000_18' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/ship_mode/_temporary/0/task_202207241800462157304366773448997_0018_m_000000
22/07/24 18:00:46 INFO SparkHadoopMapRedUtil: attempt_202207241800462157304366773448997_0018_m_000000_18: Committed
22/07/24 18:00:46 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 2353 bytes result sent to driver
22/07/24 18:00:46 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 38 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:46 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
22/07/24 18:00:46 INFO DAGScheduler: ResultStage 18 (main at NativeMethodAccessorImpl.java:0) finished in 0.048 s
22/07/24 18:00:46 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
22/07/24 18:00:46 INFO DAGScheduler: Job 18 finished: main at NativeMethodAccessorImpl.java:0, took 0.048986 s
22/07/24 18:00:46 INFO FileFormatWriter: Write Job f8e9ca34-7d70-4122-a022-f802a1ef18e3 committed.
22/07/24 18:00:46 INFO FileFormatWriter: Finished processing stats for write job f8e9ca34-7d70-4122-a022-f802a1ef18e3.
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 7.143967 ms
22/07/24 18:00:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:46 INFO DAGScheduler: Got job 19 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:46 INFO DAGScheduler: Final stage: ResultStage 19 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:46 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:46 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:46 INFO DAGScheduler: Submitting ResultStage 19 (CoalescedRDD[177] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 221.1 KiB, free 363.7 MiB)
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 68.9 KiB, free 363.7 MiB)
22/07/24 18:00:46 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 68.9 KiB, free: 365.7 MiB)
22/07/24 18:00:46 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (CoalescedRDD[177] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:46 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
22/07/24 18:00:46 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:46 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:46 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:46 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "s_store_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_store_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_rec_start_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_rec_end_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_closed_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_store_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_number_employees",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_floor_space",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_hours",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_manager",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_market_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_geography_class",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_market_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_market_manager",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_division_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_division_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_company_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_company_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_street_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_street_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_street_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_suite_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_county",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_state",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_zip",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_gmt_offset",
    "type" : "decimal(5,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "s_tax_percentage",
    "type" : "decimal(5,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 s_store_sk;
  optional binary s_store_id (UTF8);
  optional int32 s_rec_start_date (DATE);
  optional int32 s_rec_end_date (DATE);
  optional int32 s_closed_date_sk;
  optional binary s_store_name (UTF8);
  optional int32 s_number_employees;
  optional int32 s_floor_space;
  optional binary s_hours (UTF8);
  optional binary s_manager (UTF8);
  optional int32 s_market_id;
  optional binary s_geography_class (UTF8);
  optional binary s_market_desc (UTF8);
  optional binary s_market_manager (UTF8);
  optional int32 s_division_id;
  optional binary s_division_name (UTF8);
  optional int32 s_company_id;
  optional binary s_company_name (UTF8);
  optional binary s_street_number (UTF8);
  optional binary s_street_name (UTF8);
  optional binary s_street_type (UTF8);
  optional binary s_suite_number (UTF8);
  optional binary s_city (UTF8);
  optional binary s_county (UTF8);
  optional binary s_state (UTF8);
  optional binary s_zip (UTF8);
  optional binary s_country (UTF8);
  optional int32 s_gmt_offset (DECIMAL(5,2));
  optional int32 s_tax_percentage (DECIMAL(5,2));
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:46 INFO BlockManagerInfo: Removed broadcast_18_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 64.0 KiB, free: 365.7 MiB)
22/07/24 18:00:46 INFO BlockManagerInfo: Removed broadcast_16_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 66.6 KiB, free: 365.8 MiB)
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 41.152526 ms
22/07/24 18:00:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 3322
22/07/24 18:00:46 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800461562761738798817029_0019_m_000000_19' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/store/_temporary/0/task_202207241800461562761738798817029_0019_m_000000
22/07/24 18:00:46 INFO SparkHadoopMapRedUtil: attempt_202207241800461562761738798817029_0019_m_000000_19: Committed
22/07/24 18:00:46 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 2396 bytes result sent to driver
22/07/24 18:00:46 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 91 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:46 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
22/07/24 18:00:46 INFO DAGScheduler: ResultStage 19 (main at NativeMethodAccessorImpl.java:0) finished in 0.103 s
22/07/24 18:00:46 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
22/07/24 18:00:46 INFO DAGScheduler: Job 19 finished: main at NativeMethodAccessorImpl.java:0, took 0.103462 s
22/07/24 18:00:46 INFO FileFormatWriter: Write Job 2424a24c-2673-4919-8dca-2df03c6b1f30 committed.
22/07/24 18:00:46 INFO FileFormatWriter: Finished processing stats for write job 2424a24c-2673-4919-8dca-2df03c6b1f30.
22/07/24 18:00:46 INFO BlockManagerInfo: Removed broadcast_17_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 63.7 KiB, free: 365.8 MiB)
22/07/24 18:00:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 4.478858 ms
22/07/24 18:00:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:46 INFO DAGScheduler: Got job 20 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:46 INFO DAGScheduler: Final stage: ResultStage 20 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:46 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:46 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:46 INFO DAGScheduler: Submitting ResultStage 20 (CoalescedRDD[186] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 195.6 KiB, free 364.2 MiB)
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 65.3 KiB, free 364.2 MiB)
22/07/24 18:00:46 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 65.3 KiB, free: 365.8 MiB)
22/07/24 18:00:46 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (CoalescedRDD[186] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:46 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
22/07/24 18:00:46 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:46 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:46 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:46 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "t_time_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_time_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_time",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_hour",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_minute",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_second",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_am_pm",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_shift",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_sub_shift",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "t_meal_time",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 t_time_sk;
  optional binary t_time_id (UTF8);
  optional int32 t_time;
  optional int32 t_hour;
  optional int32 t_minute;
  optional int32 t_second;
  optional binary t_am_pm (UTF8);
  optional binary t_shift (UTF8);
  optional binary t_sub_shift (UTF8);
  optional binary t_meal_time (UTF8);
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 10.65388 ms
22/07/24 18:00:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 4231531
22/07/24 18:00:46 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800461810316188748916631_0020_m_000000_20' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/time_dim/_temporary/0/task_202207241800461810316188748916631_0020_m_000000
22/07/24 18:00:46 INFO SparkHadoopMapRedUtil: attempt_202207241800461810316188748916631_0020_m_000000_20: Committed
22/07/24 18:00:46 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 2396 bytes result sent to driver
22/07/24 18:00:46 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 349 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:46 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
22/07/24 18:00:46 INFO DAGScheduler: ResultStage 20 (main at NativeMethodAccessorImpl.java:0) finished in 0.361 s
22/07/24 18:00:46 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
22/07/24 18:00:46 INFO DAGScheduler: Job 20 finished: main at NativeMethodAccessorImpl.java:0, took 0.362619 s
22/07/24 18:00:46 INFO FileFormatWriter: Write Job 8f70b23a-8297-4914-afd9-ef55c2b566c5 committed.
22/07/24 18:00:46 INFO FileFormatWriter: Finished processing stats for write job 8f70b23a-8297-4914-afd9-ef55c2b566c5.
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:46 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 4.045462 ms
22/07/24 18:00:46 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:46 INFO DAGScheduler: Got job 21 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:46 INFO DAGScheduler: Final stage: ResultStage 21 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:46 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:46 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:46 INFO DAGScheduler: Submitting ResultStage 21 (CoalescedRDD[195] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 200.1 KiB, free 364.0 MiB)
22/07/24 18:00:46 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 65.8 KiB, free 363.9 MiB)
22/07/24 18:00:46 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 65.8 KiB, free: 365.7 MiB)
22/07/24 18:00:46 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (CoalescedRDD[195] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:46 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
22/07/24 18:00:46 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:46 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:46 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:46 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:46 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:46 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:46 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:46 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:46 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:46 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:46 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "w_warehouse_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_warehouse_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_warehouse_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_warehouse_sq_ft",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_street_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_street_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_street_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_suite_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_county",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_state",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_zip",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "w_gmt_offset",
    "type" : "decimal(5,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 w_warehouse_sk;
  optional binary w_warehouse_id (UTF8);
  optional binary w_warehouse_name (UTF8);
  optional int32 w_warehouse_sq_ft;
  optional binary w_street_number (UTF8);
  optional binary w_street_name (UTF8);
  optional binary w_street_type (UTF8);
  optional binary w_suite_number (UTF8);
  optional binary w_city (UTF8);
  optional binary w_county (UTF8);
  optional binary w_state (UTF8);
  optional binary w_zip (UTF8);
  optional binary w_country (UTF8);
  optional int32 w_gmt_offset (DECIMAL(5,2));
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:46 INFO CodeGenerator: Code generated in 8.92967 ms
22/07/24 18:00:46 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 786
22/07/24 18:00:46 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800466803750019079828915_0021_m_000000_21' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/warehouse/_temporary/0/task_202207241800466803750019079828915_0021_m_000000
22/07/24 18:00:46 INFO SparkHadoopMapRedUtil: attempt_202207241800466803750019079828915_0021_m_000000_21: Committed
22/07/24 18:00:46 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 2353 bytes result sent to driver
22/07/24 18:00:46 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 39 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:46 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
22/07/24 18:00:46 INFO DAGScheduler: ResultStage 21 (main at NativeMethodAccessorImpl.java:0) finished in 0.049 s
22/07/24 18:00:46 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
22/07/24 18:00:46 INFO DAGScheduler: Job 21 finished: main at NativeMethodAccessorImpl.java:0, took 0.049382 s
22/07/24 18:00:46 INFO FileFormatWriter: Write Job fb12b780-7915-4b80-8a26-7afface0131d committed.
22/07/24 18:00:46 INFO FileFormatWriter: Finished processing stats for write job fb12b780-7915-4b80-8a26-7afface0131d.
22/07/24 18:00:46 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO CodeGenerator: Code generated in 5.627971 ms
22/07/24 18:00:47 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:47 INFO DAGScheduler: Got job 22 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:47 INFO DAGScheduler: Final stage: ResultStage 22 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:47 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:47 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:47 INFO DAGScheduler: Submitting ResultStage 22 (CoalescedRDD[204] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:47 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 201.9 KiB, free 363.7 MiB)
22/07/24 18:00:47 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 66.3 KiB, free 363.7 MiB)
22/07/24 18:00:47 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 66.3 KiB, free: 365.7 MiB)
22/07/24 18:00:47 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (CoalescedRDD[204] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:47 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
22/07/24 18:00:47 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:47 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
22/07/24 18:00:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:47 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:47 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:47 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:47 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:47 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:47 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "wp_web_page_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_web_page_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_rec_start_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_rec_end_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_creation_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_access_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_autogen_flag",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_customer_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_url",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_char_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_link_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_image_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "wp_max_ad_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 wp_web_page_sk;
  optional binary wp_web_page_id (UTF8);
  optional int32 wp_rec_start_date (DATE);
  optional int32 wp_rec_end_date (DATE);
  optional int32 wp_creation_date_sk;
  optional int32 wp_access_date_sk;
  optional binary wp_autogen_flag (UTF8);
  optional int32 wp_customer_sk;
  optional binary wp_url (UTF8);
  optional binary wp_type (UTF8);
  optional int32 wp_char_count;
  optional int32 wp_link_count;
  optional int32 wp_image_count;
  optional int32 wp_max_ad_count;
}

       
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 5414
22/07/24 18:00:47 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800477664791526683556725_0022_m_000000_22' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/web_page/_temporary/0/task_202207241800477664791526683556725_0022_m_000000
22/07/24 18:00:47 INFO SparkHadoopMapRedUtil: attempt_202207241800477664791526683556725_0022_m_000000_22: Committed
22/07/24 18:00:47 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 2353 bytes result sent to driver
22/07/24 18:00:47 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 23 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:47 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
22/07/24 18:00:47 INFO DAGScheduler: ResultStage 22 (main at NativeMethodAccessorImpl.java:0) finished in 0.034 s
22/07/24 18:00:47 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
22/07/24 18:00:47 INFO DAGScheduler: Job 22 finished: main at NativeMethodAccessorImpl.java:0, took 0.034707 s
22/07/24 18:00:47 INFO FileFormatWriter: Write Job ee6733f8-5455-4e13-a2c0-4e4bc8269d45 committed.
22/07/24 18:00:47 INFO FileFormatWriter: Finished processing stats for write job ee6733f8-5455-4e13-a2c0-4e4bc8269d45.
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 WARN CharVarcharUtils: The Spark cast operator does not support char/varchar type and simply treats them as string type. Please use string type directly to avoid confusion. Otherwise, you can set spark.sql.legacy.charVarcharAsString to true, so that Spark treat them as string type as same as Spark 3.0 and earlier
22/07/24 18:00:47 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO CodeGenerator: Code generated in 7.055186 ms
22/07/24 18:00:47 INFO BlockManagerInfo: Removed broadcast_21_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 65.8 KiB, free: 365.7 MiB)
22/07/24 18:00:47 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:0
22/07/24 18:00:47 INFO DAGScheduler: Got job 23 (main at NativeMethodAccessorImpl.java:0) with 1 output partitions
22/07/24 18:00:47 INFO DAGScheduler: Final stage: ResultStage 23 (main at NativeMethodAccessorImpl.java:0)
22/07/24 18:00:47 INFO DAGScheduler: Parents of final stage: List()
22/07/24 18:00:47 INFO DAGScheduler: Missing parents: List()
22/07/24 18:00:47 INFO DAGScheduler: Submitting ResultStage 23 (CoalescedRDD[213] at main at NativeMethodAccessorImpl.java:0), which has no missing parents
22/07/24 18:00:47 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 217.2 KiB, free 363.7 MiB)
22/07/24 18:00:47 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 68.3 KiB, free 363.6 MiB)
22/07/24 18:00:47 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on ubuntu-Virtual-Machine.mshome.net:38047 (size: 68.3 KiB, free: 365.6 MiB)
22/07/24 18:00:47 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1383
22/07/24 18:00:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (CoalescedRDD[213] at main at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
22/07/24 18:00:47 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
22/07/24 18:00:47 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (ubuntu-Virtual-Machine.mshome.net, executor driver, partition 0, PROCESS_LOCAL, 4834 bytes) taskResourceAssignments Map()
22/07/24 18:00:47 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
22/07/24 18:00:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:47 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
22/07/24 18:00:47 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
22/07/24 18:00:47 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:47 INFO CodecConfig: Compression: SNAPPY
22/07/24 18:00:47 INFO ParquetOutputFormat: Parquet block size to 134217728
22/07/24 18:00:47 INFO ParquetOutputFormat: Parquet page size to 1048576
22/07/24 18:00:47 INFO ParquetOutputFormat: Parquet dictionary page size to 1048576
22/07/24 18:00:47 INFO ParquetOutputFormat: Dictionary is on
22/07/24 18:00:47 INFO ParquetOutputFormat: Validation is off
22/07/24 18:00:47 INFO ParquetOutputFormat: Writer version is: PARQUET_1_0
22/07/24 18:00:47 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
22/07/24 18:00:47 INFO ParquetOutputFormat: Page size checking is: estimated
22/07/24 18:00:47 INFO ParquetOutputFormat: Min row count for page size check is: 100
22/07/24 18:00:47 INFO ParquetOutputFormat: Max row count for page size check is: 10000
22/07/24 18:00:47 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "web_site_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_site_id",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_rec_start_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_rec_end_date",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_open_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_close_date_sk",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_class",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_manager",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_mkt_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_mkt_class",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_mkt_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_market_manager",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_company_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_company_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_street_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_street_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_street_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_suite_number",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_city",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_county",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_state",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_zip",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_country",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_gmt_offset",
    "type" : "decimal(5,2)",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "web_tax_percentage",
    "type" : "decimal(5,2)",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int32 web_site_sk;
  optional binary web_site_id (UTF8);
  optional int32 web_rec_start_date (DATE);
  optional int32 web_rec_end_date (DATE);
  optional binary web_name (UTF8);
  optional int32 web_open_date_sk;
  optional int32 web_close_date_sk;
  optional binary web_class (UTF8);
  optional binary web_manager (UTF8);
  optional int32 web_mkt_id;
  optional binary web_mkt_class (UTF8);
  optional binary web_mkt_desc (UTF8);
  optional binary web_market_manager (UTF8);
  optional int32 web_company_id;
  optional binary web_company_name (UTF8);
  optional binary web_street_number (UTF8);
  optional binary web_street_name (UTF8);
  optional binary web_street_type (UTF8);
  optional binary web_suite_number (UTF8);
  optional binary web_city (UTF8);
  optional binary web_county (UTF8);
  optional binary web_state (UTF8);
  optional binary web_zip (UTF8);
  optional binary web_country (UTF8);
  optional int32 web_gmt_offset (DECIMAL(5,2));
  optional int32 web_tax_percentage (DECIMAL(5,2));
}

       
22/07/24 18:00:47 INFO BlockManagerInfo: Removed broadcast_22_piece0 on ubuntu-Virtual-Machine.mshome.net:38047 in memory (size: 66.3 KiB, free: 365.7 MiB)
DBGEN2 Population Generator (Version 1.0.0h (pre-release))
Copyright Transaction Processing Performance Council (TPC) 2001 - 2008
Warning: This scale factor is valid for QUALIFICATION ONLY
22/07/24 18:00:47 INFO CodeGenerator: Code generated in 14.241948 ms
22/07/24 18:00:47 INFO InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 8386
22/07/24 18:00:47 INFO FileOutputCommitter: Saved output of task 'attempt_202207241800473752427891236578435_0023_m_000000_23' to file:/home/ubuntu/git/spark-tpcds-datagen/tpcds-data-1g/web_site/_temporary/0/task_202207241800473752427891236578435_0023_m_000000
22/07/24 18:00:47 INFO SparkHadoopMapRedUtil: attempt_202207241800473752427891236578435_0023_m_000000_23: Committed
22/07/24 18:00:47 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 2353 bytes result sent to driver
22/07/24 18:00:47 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 65 ms on ubuntu-Virtual-Machine.mshome.net (executor driver) (1/1)
22/07/24 18:00:47 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
22/07/24 18:00:47 INFO DAGScheduler: ResultStage 23 (main at NativeMethodAccessorImpl.java:0) finished in 0.079 s
22/07/24 18:00:47 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
22/07/24 18:00:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
22/07/24 18:00:47 INFO DAGScheduler: Job 23 finished: main at NativeMethodAccessorImpl.java:0, took 0.080125 s
22/07/24 18:00:47 INFO FileFormatWriter: Write Job b33f7b8b-9100-4d8b-a89d-d1a7d7b44733 committed.
22/07/24 18:00:47 INFO FileFormatWriter: Finished processing stats for write job b33f7b8b-9100-4d8b-a89d-d1a7d7b44733.
22/07/24 18:00:47 INFO SparkUI: Stopped Spark web UI at http://ubuntu-Virtual-Machine.mshome.net:4040
22/07/24 18:00:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/07/24 18:00:47 INFO MemoryStore: MemoryStore cleared
22/07/24 18:00:47 INFO BlockManager: BlockManager stopped
22/07/24 18:00:47 INFO BlockManagerMaster: BlockManagerMaster stopped
22/07/24 18:00:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/07/24 18:00:47 INFO SparkContext: Successfully stopped SparkContext
22/07/24 18:00:47 INFO ShutdownHookManager: Shutdown hook called
22/07/24 18:00:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe9b1ba4-3ef1-4061-ad6d-b15f9243ba66
22/07/24 18:00:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-f7358032-96fd-4933-9317-4811dbd3c8c8
ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ echo $?
0
ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ wget https://repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.12/3.1.1/spark-catalyst_2.12-3.1.1-tests.jar
--2022-07-24 18:05:36--  https://repo1.maven.org/maven2/org/apache/spark/spark-catalyst_2.12/3.1.1/spark-catalyst_2.12-3.1.1-tests.jar
Resolving repo1.maven.org (repo1.maven.org)... 199.232.196.209, 199.232.192.209
Connecting to repo1.maven.org (repo1.maven.org)|199.232.196.209|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4642097 (4.4M) [application/java-archive]
Saving to: ‘spark-catalyst_2.12-3.1.1-tests.jar’

spark-catalyst_2.12-3.1.1-t 100%[==========================================>]   4.43M  3.52MB/s    in 1.3s    

2022-07-24 18:05:39 (3.52 MB/s) - ‘spark-catalyst_2.12-3.1.1-tests.jar’ saved [4642097/4642097]

ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ wget https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/3.1.1/spark-core_2.12-3.1.1-tests.jar
--2022-07-24 18:06:50--  https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/3.1.1/spark-core_2.12-3.1.1-tests.jar
Resolving repo1.maven.org (repo1.maven.org)... 199.232.196.209, 199.232.192.209
Connecting to repo1.maven.org (repo1.maven.org)|199.232.196.209|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 4860916 (4.6M) [application/java-archive]
Saving to: ‘spark-core_2.12-3.1.1-tests.jar’

spark-core_2.12-3.1.1-tests 100%[==========================================>]   4.63M  3.93MB/s    in 1.2s    

2022-07-24 18:06:52 (3.93 MB/s) - ‘spark-core_2.12-3.1.1-tests.jar’ saved [4860916/4860916]

ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/3.1.1/spark-sql_2.12-3.1.1-tests.jar
--2022-07-24 18:07:39--  https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.12/3.1.1/spark-sql_2.12-3.1.1-tests.jar
Resolving repo1.maven.org (repo1.maven.org)... 199.232.196.209, 199.232.192.209
Connecting to repo1.maven.org (repo1.maven.org)|199.232.196.209|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 15921702 (15M) [application/java-archive]
Saving to: ‘spark-sql_2.12-3.1.1-tests.jar’

spark-sql_2.12-3.1.1-tests. 100%[==========================================>]  15.18M  8.74MB/s    in 1.7s    

2022-07-24 18:07:42 (8.74 MB/s) - ‘spark-sql_2.12-3.1.1-tests.jar’ saved [15921702/15921702]

ubuntu@ubuntu-Virtual-Machine:~/git/spark-tpcds-datagen$ 
